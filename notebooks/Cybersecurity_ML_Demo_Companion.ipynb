{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ›¡ï¸ Cybersecurity ML Demo Companion\n",
        "\n",
        "## Enterprise ML Training & Model Registry Integration\n",
        "\n",
        "This notebook is the **ML training companion** for the Snowflake Cybersecurity Demo. It trains and deploys **real machine learning models** using enterprise-grade practices:\n",
        "\n",
        "- **ğŸŒ² Isolation Forest** - Production anomaly detection algorithms\n",
        "- **ğŸ¯ K-means Clustering** - Behavioral user classification  \n",
        "- **ğŸ“š Model Registry** - Enterprise model lifecycle management\n",
        "- **ğŸš€ Auto-UDF Deployment** - Seamless production integration\n",
        "\n",
        "### ğŸ¯ Purpose & Usage\n",
        "This notebook transforms the demo from **simulated ML** to **production-grade ML**:\n",
        "- **Run Once**: Train models and register in Model Registry\n",
        "- **Enterprise Ready**: Version control, metadata, governance\n",
        "- **Auto-Deploy**: Models become SQL UDFs automatically\n",
        "- **Production Use**: Streamlit demo uses real ML immediately\n",
        "\n",
        "### ğŸ“‹ Prerequisites\n",
        "1. âœ… Completed SQL setup: `01_cybersecurity_schema.sql`, `02_sample_data_generation.sql`, `03_ai_ml_models.sql`\n",
        "2. âœ… Snowflake account with ACCOUNTADMIN privileges\n",
        "3. âœ… Model Registry access (included with Snowflake Notebooks)\n",
        "4. âœ… Python packages: `snowflake-ml-python`, `scikit-learn`, `pandas`, `numpy`\n",
        "\n",
        "### ğŸ”„ Workflow\n",
        "1. **Train Models**: Extract features, train ML algorithms\n",
        "2. **Register Models**: Store in Model Registry with metadata\n",
        "3. **Auto-Deploy**: Models become SQL UDFs automatically  \n",
        "4. **Use in Demo**: Streamlit queries real ML models\n",
        "5. **Retrain**: Run notebook again for model updates\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”§ 1. Setup and Configuration\n",
        "\n",
        "First, let's import the necessary libraries and establish our Snowflake connection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import snowflake.snowpark as snowpark\n",
        "from snowflake.snowpark import Session\n",
        "from snowflake.snowpark.types import StructType, StructField, StringType, FloatType, IntegerType\n",
        "from snowflake.snowpark import context\n",
        "from snowflake.ml.registry import Registry\n",
        "from snowflake.ml.model import Model\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "import logging\n",
        "from typing import Tuple, Dict, Any\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"âœ… Libraries imported successfully!\")\n",
        "print(\"ğŸ“š Snowflake Model Registry integration enabled!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”‘ 2. Snowflake Session Setup\n",
        "\n",
        "**âœ… Snowflake Notebooks**: This notebook is designed for Snowflake Notebooks where the session is automatically provided.\n",
        "\n",
        "For local development, you can manually create a session with your credentials.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get Snowflake session\n",
        "# In Snowflake Notebooks, the session is automatically available\n",
        "try:\n",
        "    # For Snowflake Notebooks - session is provided automatically\n",
        "    session = context.get_active_session()\n",
        "    print(\"âœ… Using Snowflake Notebooks session\")\n",
        "    session_type = \"snowflake_notebooks\"\n",
        "except:\n",
        "    # For local development - create session manually\n",
        "    print(\"ğŸ”§ Creating manual session for local development\")\n",
        "    session_type = \"local_development\"\n",
        "    \n",
        "    # Uncomment and update these parameters for local development:\n",
        "    # connection_parameters = {\n",
        "    #     \"account\": \"your_account\",\n",
        "    #     \"user\": \"your_username\",  \n",
        "    #     \"password\": \"your_password\",\n",
        "    #     \"role\": \"ACCOUNTADMIN\",\n",
        "    #     \"warehouse\": \"COMPUTE_WH\",\n",
        "    #     \"database\": \"CYBERSECURITY_DEMO\",\n",
        "    #     \"schema\": \"SECURITY_AI\"\n",
        "    # }\n",
        "    # session = Session.builder.configs(connection_parameters).create()\n",
        "    \n",
        "    print(\"âŒ No session available. Please configure connection_parameters above for local development.\")\n",
        "    session = None\n",
        "\n",
        "if session:\n",
        "    print(f\"ğŸ“Š Session type: {session_type}\")\n",
        "else:\n",
        "    print(\"âš ï¸  No active session. Please configure connection for local development.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Snowflake session and set context\n",
        "if session:\n",
        "    try:\n",
        "        # Set the correct database and schema context\n",
        "        session.sql(\"USE DATABASE CYBERSECURITY_DEMO\").collect()\n",
        "        session.sql(\"USE SCHEMA SECURITY_AI\").collect()\n",
        "        \n",
        "        # Test connection with a simple query\n",
        "        result = session.sql(\"SELECT CURRENT_DATABASE(), CURRENT_SCHEMA(), CURRENT_USER()\").collect()\n",
        "        print(f\"âœ… Session active and connected!\")\n",
        "        print(f\"ğŸ” Current context: {result[0][0]}.{result[0][1]} as {result[0][2]}\")\n",
        "        \n",
        "        session_ready = True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Session test failed: {str(e)}\")\n",
        "        print(\"ğŸ”§ Please ensure the CYBERSECURITY_DEMO database and SECURITY_AI schema exist.\")\n",
        "        session_ready = False\n",
        "else:\n",
        "    print(\"âŒ No session available\")\n",
        "    session_ready = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“š 3. Snowflake Model Registry Setup\n",
        "\n",
        "**âœ¨ Enterprise Model Management**: Set up the Snowflake Model Registry for professional ML model lifecycle management.\n",
        "\n",
        "### Benefits:\n",
        "- **ğŸ“ Version Control**: Track model versions and changes\n",
        "- **ğŸ“Š Metadata Management**: Store training metrics and model information  \n",
        "- **ğŸ”’ Access Control**: Role-based permissions for model access\n",
        "- **ğŸ”„ Model Lineage**: Track model relationships and dependencies\n",
        "- **ğŸš€ Auto-Deployment**: Seamless deployment as UDFs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Snowflake Model Registry\n",
        "if session_ready:\n",
        "    try:\n",
        "        # Initialize the Model Registry\n",
        "        registry = Registry(\n",
        "            session=session,\n",
        "            database_name=\"CYBERSECURITY_DEMO\",\n",
        "            schema_name=\"SECURITY_AI\"\n",
        "        )\n",
        "        \n",
        "        print(\"âœ… Model Registry initialized successfully!\")\n",
        "        print(f\"ğŸ“ Registry location: CYBERSECURITY_DEMO.SECURITY_AI\")\n",
        "        \n",
        "        # List existing models (if any)\n",
        "        try:\n",
        "            models = registry.show_models()\n",
        "            if len(models) > 0:\n",
        "                print(f\"ğŸ“š Found {len(models)} existing models in registry\")\n",
        "                for model in models:\n",
        "                    print(f\"  ğŸ“– {model}\")\n",
        "            else:\n",
        "                print(\"ğŸ“ Registry is empty - ready for new models\")\n",
        "        except:\n",
        "            print(\"ğŸ“ Registry initialized - ready for first models\")\n",
        "            \n",
        "        registry_ready = True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Model Registry initialization failed: {str(e)}\")\n",
        "        print(\"ğŸ’¡ Ensure you have proper permissions and snowflake-ml-python is installed\")\n",
        "        registry_ready = False\n",
        "        registry = None\n",
        "else:\n",
        "    print(\"âŒ Skipping Model Registry setup - session not ready\")\n",
        "    registry_ready = False\n",
        "    registry = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“Š 4. Data Validation and Readiness Check\n",
        "\n",
        "Before training ML models, let's validate that we have sufficient, high-quality data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check ML training data readiness\n",
        "def validate_training_data(session: Session) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Validate that sufficient data exists for ML training\n",
        "    \"\"\"\n",
        "    print(\"ğŸ” Validating training data readiness...\")\n",
        "    \n",
        "    try:\n",
        "        # Check overall data volume\n",
        "        validation_query = \"\"\"\n",
        "        SELECT \n",
        "            COUNT(*) as total_events,\n",
        "            COUNT(DISTINCT username) as unique_users,\n",
        "            COUNT(DISTINCT DATE(timestamp)) as training_days,\n",
        "            ROUND(COUNT(*) / COUNT(DISTINCT username), 2) as avg_events_per_user,\n",
        "            COUNT(DISTINCT location:country::STRING) as unique_countries,\n",
        "            COUNT(DISTINCT source_ip) as unique_ips,\n",
        "            ROUND(AVG(CASE WHEN success THEN 1.0 ELSE 0.0 END), 3) as success_rate\n",
        "        FROM USER_AUTHENTICATION_LOGS\n",
        "        WHERE timestamp >= DATEADD(day, -90, CURRENT_TIMESTAMP())\n",
        "        \"\"\"\n",
        "        \n",
        "        result = session.sql(validation_query).collect()[0]\n",
        "        \n",
        "        metrics = {\n",
        "            'total_events': result['TOTAL_EVENTS'],\n",
        "            'unique_users': result['UNIQUE_USERS'], \n",
        "            'training_days': result['TRAINING_DAYS'],\n",
        "            'avg_events_per_user': result['AVG_EVENTS_PER_USER'],\n",
        "            'unique_countries': result['UNIQUE_COUNTRIES'],\n",
        "            'unique_ips': result['UNIQUE_IPS'],\n",
        "            'success_rate': result['SUCCESS_RATE']\n",
        "        }\n",
        "        \n",
        "        return metrics\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Data validation failed: {str(e)}\")\n",
        "        return {}\n",
        "\n",
        "# Run validation\n",
        "if session_ready:\n",
        "    data_metrics = validate_training_data(session)\n",
        "else:\n",
        "    data_metrics = {}\n",
        "\n",
        "if data_metrics:\n",
        "    print(\"\\nğŸ“Š Training Data Summary:\")\n",
        "    print(f\"  ğŸ“ˆ Total Events: {data_metrics['total_events']:,}\")\n",
        "    print(f\"  ğŸ‘¥ Unique Users: {data_metrics['unique_users']:,}\")\n",
        "    print(f\"  ğŸ“… Training Days: {data_metrics['training_days']}\")\n",
        "    print(f\"  ğŸ“Š Avg Events/User: {data_metrics['avg_events_per_user']}\")\n",
        "    print(f\"  ğŸŒ Countries: {data_metrics['unique_countries']}\")\n",
        "    print(f\"  ğŸŒ Unique IPs: {data_metrics['unique_ips']:,}\")\n",
        "    print(f\"  âœ… Success Rate: {data_metrics['success_rate']:.1%}\")\n",
        "    \n",
        "    # Determine readiness\n",
        "    if (data_metrics['total_events'] >= 100000 and \n",
        "        data_metrics['unique_users'] >= 100 and \n",
        "        data_metrics['training_days'] >= 60):\n",
        "        print(\"\\nâœ… Data is READY for ML training!\")\n",
        "        training_ready = True\n",
        "    elif (data_metrics['total_events'] >= 10000 and \n",
        "          data_metrics['unique_users'] >= 50):\n",
        "        print(\"\\nâš ï¸  Data is MINIMAL but usable for ML training.\")\n",
        "        training_ready = True\n",
        "    else:\n",
        "        print(\"\\nâŒ INSUFFICIENT data for ML training.\")\n",
        "        print(\"   Please ensure you've run the sample data generation script.\")\n",
        "        training_ready = False\n",
        "else:\n",
        "    training_ready = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¤– 5. Enhanced ML Training with Model Registry\n",
        "\n",
        "### âš ï¸ **Important: Model Registry Deployment Strategy**\n",
        "\n",
        "**After running this cell, your models will be:**\n",
        "- âœ… **Registered** in Snowflake Model Registry with version control\n",
        "- âœ… **Auto-deployed** as UDFs (e.g., `CYBERSECURITY_ISOLATION_FOREST_PREDICT`)\n",
        "- âœ… **Ready for production** use in the Streamlit demo\n",
        "- âœ… **Persistent** - no need to re-run unless retraining\n",
        "\n",
        "### ğŸ”„ **When to Re-run This Notebook:**\n",
        "- **New training data** available (monthly/quarterly retraining)\n",
        "- **Model performance** degradation detected\n",
        "- **Algorithm updates** or hyperparameter tuning needed\n",
        "- **New model versions** for A/B testing\n",
        "\n",
        "### ğŸ¯ **Training Pipeline:**\n",
        "1. Extract user behavior features from 90+ days of data\n",
        "2. Train Isolation Forest for anomaly detection  \n",
        "3. Train K-means for user clustering\n",
        "4. **Register models in Snowflake Model Registry** ğŸ“š\n",
        "5. **Deploy models as versioned UDFs** ğŸš€\n",
        "6. Add metadata and performance tracking\n",
        "\n",
        "**Run this cell to train and deploy your real ML models!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if session_ready and training_ready and registry_ready:\n",
        "    print(\"ğŸš€ Starting complete ML training and deployment pipeline...\")\n",
        "    \n",
        "    # 1. Extract user behavior features\n",
        "    print(\"\\nğŸ“Š Step 1: Feature Extraction\")\n",
        "    feature_query = \"\"\"\n",
        "    SELECT \n",
        "        username,\n",
        "        AVG(EXTRACT(HOUR FROM timestamp)) as avg_login_hour,\n",
        "        COALESCE(STDDEV(EXTRACT(HOUR FROM timestamp)), 0) as stddev_login_hour,\n",
        "        COUNT(*) as total_logins,\n",
        "        COUNT(DISTINCT source_ip) as unique_ips,\n",
        "        COUNT(DISTINCT location:country::STRING) as countries,\n",
        "        AVG(CASE WHEN EXTRACT(DOW FROM timestamp) IN (0,6) THEN 1.0 ELSE 0.0 END) as weekend_ratio,\n",
        "        AVG(CASE WHEN EXTRACT(HOUR FROM timestamp) BETWEEN 22 AND 6 THEN 1.0 ELSE 0.0 END) as offhours_ratio\n",
        "    FROM USER_AUTHENTICATION_LOGS\n",
        "    WHERE timestamp >= DATEADD(day, -90, CURRENT_TIMESTAMP())\n",
        "      AND username IS NOT NULL\n",
        "    GROUP BY username\n",
        "    HAVING COUNT(*) >= 10\n",
        "    \"\"\"\n",
        "    \n",
        "    training_df = session.sql(feature_query).to_pandas().fillna(0)\n",
        "    print(f\"âœ… Extracted features for {len(training_df)} users\")\n",
        "    \n",
        "    # 2. Train Isolation Forest\n",
        "    print(\"\\nğŸŒ² Step 2: Training Isolation Forest\")\n",
        "    feature_cols = ['avg_login_hour', 'stddev_login_hour', 'unique_ips', 'countries', 'weekend_ratio', 'offhours_ratio']\n",
        "    X = training_df[feature_cols]\n",
        "    \n",
        "    # Standardize features\n",
        "    isolation_scaler = StandardScaler()\n",
        "    X_scaled = isolation_scaler.fit_transform(X)\n",
        "    \n",
        "    # Train model\n",
        "    isolation_model = IsolationForest(contamination=0.1, random_state=42, n_estimators=100)\n",
        "    isolation_model.fit(X_scaled)\n",
        "    \n",
        "    # Get results\n",
        "    scores = isolation_model.decision_function(X_scaled)\n",
        "    anomalies = isolation_model.predict(X_scaled)\n",
        "    n_anomalies = sum(anomalies == -1)\n",
        "    \n",
        "    print(f\"âœ… Isolation Forest trained: {n_anomalies} anomalies detected ({n_anomalies/len(training_df):.1%})\")\n",
        "    \n",
        "    # 3. Train K-means\n",
        "    print(\"\\nğŸ¯ Step 3: Training K-means Clustering\") \n",
        "    cluster_features = ['avg_login_hour', 'countries', 'weekend_ratio', 'offhours_ratio', 'unique_ips']\n",
        "    X_cluster = training_df[cluster_features]\n",
        "    \n",
        "    # Standardize features\n",
        "    kmeans_scaler = StandardScaler()\n",
        "    X_cluster_scaled = kmeans_scaler.fit_transform(X_cluster)\n",
        "    \n",
        "    # Train model\n",
        "    kmeans_model = KMeans(n_clusters=6, random_state=42, n_init=10)\n",
        "    clusters = kmeans_model.fit_predict(X_cluster_scaled)\n",
        "    \n",
        "    print(f\"âœ… K-means trained: {len(np.unique(clusters))} behavioral clusters created\")\n",
        "    \n",
        "    # 4. Register models in Snowflake Model Registry\n",
        "    print(\"\\nğŸ“š Step 4: Registering Models in Model Registry\")\n",
        "    \n",
        "    # Create sample input data for model signatures\n",
        "    sample_input = X.iloc[:5]  # First 5 rows for model signature\n",
        "    \n",
        "    # Generate model version with timestamp\n",
        "    model_version = f\"v_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    \n",
        "    # Register Isolation Forest model\n",
        "    print(\"  ğŸŒ² Registering Isolation Forest model...\")\n",
        "    isolation_model_ref = registry.log_model(\n",
        "        model_name=\"cybersecurity_isolation_forest\",\n",
        "        version_name=model_version,\n",
        "        model=isolation_model,\n",
        "        sample_input_data=sample_input,\n",
        "        metadata={\n",
        "            \"model_type\": \"anomaly_detection\",\n",
        "            \"algorithm\": \"isolation_forest\",\n",
        "            \"contamination\": 0.1,\n",
        "            \"n_estimators\": 100,\n",
        "            \"training_samples\": len(training_df),\n",
        "            \"features\": feature_cols,\n",
        "            \"anomalies_detected\": n_anomalies,\n",
        "            \"anomaly_rate\": f\"{n_anomalies/len(training_df):.1%}\",\n",
        "            \"trained_at\": datetime.now().isoformat(),\n",
        "            \"purpose\": \"cybersecurity_user_behavior_anomaly_detection\"\n",
        "        }\n",
        "    )\n",
        "    print(f\"  âœ… Isolation Forest registered as {model_version}\")\n",
        "    \n",
        "    # Register K-means model  \n",
        "    print(\"  ğŸ¯ Registering K-means clustering model...\")\n",
        "    cluster_sample = X_cluster.iloc[:5]  # Sample for clustering model\n",
        "    kmeans_model_ref = registry.log_model(\n",
        "        model_name=\"cybersecurity_kmeans_clustering\", \n",
        "        version_name=model_version,\n",
        "        model=kmeans_model,\n",
        "        sample_input_data=cluster_sample,\n",
        "        metadata={\n",
        "            \"model_type\": \"clustering\",\n",
        "            \"algorithm\": \"kmeans\",\n",
        "            \"n_clusters\": 6,\n",
        "            \"n_init\": 10,\n",
        "            \"training_samples\": len(training_df),\n",
        "            \"features\": cluster_features,\n",
        "            \"trained_at\": datetime.now().isoformat(),\n",
        "            \"purpose\": \"cybersecurity_user_behavior_clustering\"\n",
        "        }\n",
        "    )\n",
        "    print(f\"  âœ… K-means registered as {model_version}\")\n",
        "    \n",
        "    # Register scalers as well (important for preprocessing)\n",
        "    print(\"  ğŸ“ Registering feature scalers...\")\n",
        "    scaler_sample = X.iloc[:1]  # Single row for scaler\n",
        "    isolation_scaler_ref = registry.log_model(\n",
        "        model_name=\"cybersecurity_isolation_scaler\",\n",
        "        version_name=model_version, \n",
        "        model=isolation_scaler,\n",
        "        sample_input_data=scaler_sample,\n",
        "        metadata={\n",
        "            \"model_type\": \"preprocessor\",\n",
        "            \"scaler_type\": \"StandardScaler\",\n",
        "            \"purpose\": \"isolation_forest_feature_scaling\"\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    kmeans_scaler_ref = registry.log_model(\n",
        "        model_name=\"cybersecurity_kmeans_scaler\",\n",
        "        version_name=model_version,\n",
        "        model=kmeans_scaler, \n",
        "        sample_input_data=cluster_sample,\n",
        "        metadata={\n",
        "            \"model_type\": \"preprocessor\", \n",
        "            \"scaler_type\": \"StandardScaler\",\n",
        "            \"purpose\": \"kmeans_feature_scaling\"\n",
        "        }\n",
        "    )\n",
        "    print(f\"  âœ… Feature scalers registered\")\n",
        "    \n",
        "    # 5. Deploy models as UDFs (automatic with Model Registry)\n",
        "    print(\"\\nğŸš€ Step 5: Deploying Models as UDFs\")\n",
        "    try:\n",
        "        # Deploy Isolation Forest for inference\n",
        "        print(\"  ğŸŒ² Deploying Isolation Forest UDF...\")\n",
        "        isolation_model_ref.create_udf(\n",
        "            udf_name=\"cybersecurity_isolation_forest_predict\",\n",
        "            replace_if_exists=True\n",
        "        )\n",
        "        \n",
        "        # Deploy K-means for inference  \n",
        "        print(\"  ğŸ¯ Deploying K-means UDF...\")\n",
        "        kmeans_model_ref.create_udf(\n",
        "            udf_name=\"cybersecurity_kmeans_predict\", \n",
        "            replace_if_exists=True\n",
        "        )\n",
        "        \n",
        "        print(\"  âœ… Models deployed as UDFs successfully!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  âš ï¸  UDF deployment: {str(e)}\")\n",
        "        print(\"  ğŸ’¡ UDFs can be created manually from registered models\")\n",
        "        \n",
        "    # 6. Model Registry Validation\n",
        "    print(\"\\nğŸ” Step 6: Model Registry Validation\")\n",
        "    try:\n",
        "        # List all models in registry\n",
        "        models = registry.show_models()\n",
        "        print(f\"âœ… {len(models)} models registered in Model Registry\")\n",
        "        \n",
        "        # Show model details\n",
        "        for model_name in [\"cybersecurity_isolation_forest\", \"cybersecurity_kmeans_clustering\"]:\n",
        "            try:\n",
        "                model_info = registry.get_model(model_name)\n",
        "                print(f\"  ğŸ“– {model_name}: {model_info}\")\n",
        "            except:\n",
        "                print(f\"  âš ï¸  Model {model_name} not found\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Registry validation error: {str(e)}\")\n",
        "    \n",
        "    # Final summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ğŸ‰ ENTERPRISE ML IMPLEMENTATION WITH MODEL REGISTRY COMPLETE!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"ğŸ“Š Training Data: {len(training_df):,} users\")\n",
        "    print(f\"ğŸŒ² Isolation Forest: {n_anomalies} anomalies ({n_anomalies/len(training_df):.1%})\")\n",
        "    print(f\"ğŸ¯ K-means: {len(np.unique(clusters))} behavioral clusters\") \n",
        "    print(f\"ğŸ“š Model Registry: âœ… 4 models registered with metadata\")\n",
        "    print(f\"ğŸš€ UDF Deployment: âœ… Models available as SQL functions\")\n",
        "    print(f\"ğŸ“ Model Version: {model_version}\")\n",
        "    print(\"\\nâœ¨ Model Registry Benefits:\")\n",
        "    print(\"  ğŸ“ Version control and lineage tracking\")\n",
        "    print(\"  ğŸ“Š Rich metadata and performance metrics\")\n",
        "    print(\"  ğŸ”’ Role-based access control\") \n",
        "    print(\"  ğŸ”„ Automated deployment pipeline\")\n",
        "    print(\"\\nğŸ¯ Next Steps:\")\n",
        "    print(\"1. Test models: SELECT cybersecurity_isolation_forest_predict(...)\")\n",
        "    print(\"2. Update UDFs in SQL scripts to use Registry models\")\n",
        "    print(\"3. Launch: Your Streamlit app now uses Enterprise ML!\")\n",
        "    print(\"\\nğŸš€ This is production-grade, enterprise ML with full lifecycle management!\")\n",
        "    \n",
        "    # Post-deployment guidance\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ğŸ“‹ NEXT STEPS AFTER MODEL DEPLOYMENT\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"1. ğŸ¯ Your Streamlit demo now uses REAL ML models automatically\")\n",
        "    print(\"2. ğŸ“Š Models are persistent - no need to re-run this notebook regularly\")\n",
        "    print(\"3. ğŸ” Use 'SHOW MODELS IN MODEL REGISTRY' to view your models\")\n",
        "    print(\"4. ğŸ“ˆ Monitor model performance in production\")\n",
        "    print(\"5. ğŸ”„ Re-run this notebook only for model updates/retraining\")\n",
        "    print(\"\\nğŸ’¡ TIP: You can now focus on using the Streamlit demo!\")\n",
        "    print(\"   The heavy ML work is done and deployed.\")\n",
        "    \n",
        "else:\n",
        "    if not session_ready:\n",
        "        print(\"âŒ Skipping ML training due to session issues.\")\n",
        "        print(\"ğŸ’¡ Please ensure Snowflake session is properly configured.\")\n",
        "    elif not training_ready:\n",
        "        print(\"âŒ Skipping ML training due to insufficient data.\")\n",
        "        print(\"ğŸ’¡ Please run the SQL data generation scripts first.\")\n",
        "    elif not registry_ready:\n",
        "        print(\"âŒ Skipping ML training due to Model Registry issues.\")\n",
        "        print(\"ğŸ’¡ Please ensure snowflake-ml-python is installed and permissions are correct.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“š 6. Model Registry Benefits & Best Practices\n",
        "\n",
        "### âœ¨ **Enterprise ML Lifecycle Management**\n",
        "\n",
        "Your models are now managed with enterprise-grade practices:\n",
        "\n",
        "#### **ğŸ”’ Governance & Security**\n",
        "- **Role-based Access**: Control who can view/modify models\n",
        "- **Audit Trails**: Track all model changes and deployments\n",
        "- **Version Control**: Rollback to previous model versions\n",
        "- **Metadata Management**: Rich model documentation and lineage\n",
        "\n",
        "#### **ğŸš€ Operational Excellence**\n",
        "- **Auto-Deployment**: Models become UDFs automatically\n",
        "- **Performance Tracking**: Monitor model accuracy over time\n",
        "- **A/B Testing**: Deploy multiple model versions simultaneously  \n",
        "- **CI/CD Integration**: Automated model deployment pipelines\n",
        "\n",
        "#### **ğŸ‘¥ Team Collaboration**\n",
        "- **Model Sharing**: Team access to registered models\n",
        "- **Documentation**: Built-in model descriptions and metrics\n",
        "- **Change Management**: Track who trained/deployed which models\n",
        "- **Knowledge Transfer**: Onboard new team members easily\n",
        "\n",
        "### ğŸ¯ **Recommended Workflow**\n",
        "\n",
        "1. **Initial Setup**: Run this notebook once to train and register models\n",
        "2. **Production Use**: Streamlit demo automatically uses registered models\n",
        "3. **Monitoring**: Track model performance in production dashboards\n",
        "4. **Retraining**: Re-run notebook monthly/quarterly for fresh models\n",
        "5. **Version Management**: Use Model Registry to manage model lifecycle\n",
        "\n",
        "### ğŸ’¡ **Pro Tips**\n",
        "- Models persist across Snowflake sessions - no need for frequent retraining\n",
        "- Use versioning for gradual model rollouts and A/B testing\n",
        "- Monitor data drift and retrain models when performance degrades\n",
        "- Leverage Model Registry metadata for model documentation and compliance\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
