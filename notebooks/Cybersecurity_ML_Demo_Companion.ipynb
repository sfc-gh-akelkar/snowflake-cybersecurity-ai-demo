{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udee1\ufe0f Cybersecurity AI Demo - Complete Deployment & ML Training\n",
        "\n",
        "## \ud83d\ude80 One-Stop Setup for Snowflake Cybersecurity Demo\n",
        "\n",
        "This notebook provides **complete deployment** of the Snowflake Cybersecurity AI Demo, including:\n",
        "\n",
        "### \ud83d\udcca **Database & Schema Setup**\n",
        "- \u2705 Creates database, schema, tables, and sample data\n",
        "- \u2705 500+ users, 180+ days of realistic security telemetry\n",
        "- \u2705 Authentication logs, network security, vulnerabilities, threat intel\n",
        "\n",
        "### \ud83e\udde0 **Enterprise ML Pipeline**\n",
        "- \u2705 **Isolation Forest** - Production anomaly detection algorithms\n",
        "- \u2705 **K-means Clustering** - Behavioral user classification  \n",
        "- \u2705 **Model Registry** - Enterprise model lifecycle management\n",
        "- \u2705 **Auto-UDF Deployment** - Seamless production integration\n",
        "\n",
        "### \ud83c\udfaf **Complete Platform**\n",
        "- \u2705 **Native ML Models** - Time-series anomaly detection\n",
        "- \u2705 **Snowpark ML Models** - Custom Python algorithms\n",
        "- \u2705 **Hybrid Analytics** - Model comparison and ensemble scoring\n",
        "- \u2705 **Cortex AI Integration** - Data-driven chatbot responses\n",
        "\n",
        "### \ud83d\udccb Prerequisites\n",
        "- \u2705 Snowflake account with `ACCOUNTADMIN` privileges\n",
        "- \u2705 Model Registry access (included with Snowflake Notebooks)\n",
        "- \u2705 Python packages: `snowflake-ml-python`, `scikit-learn`, `pandas`, `numpy`\n",
        "\n",
        "### \u26a1 Quick Start\n",
        "**Just run all cells in order!** This notebook will:\n",
        "1. **Setup Database** - Create schema, tables, and sample data\n",
        "2. **Deploy ML Models** - Train and register production models\n",
        "3. **Enable AI Features** - Cortex AI and advanced analytics\n",
        "4. **Validate Deployment** - Confirm everything is working\n",
        "\n",
        "**Total time: ~15 minutes** \u23f1\ufe0f\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udcda Import Required Libraries\n",
        "\n",
        "Import all necessary Python libraries for ML training, data processing, and Snowflake integration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import core libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, Any, Optional, Tuple, List\n",
        "\n",
        "# Import Snowflake libraries\n",
        "import snowflake.snowpark as snowpark\n",
        "from snowflake.snowpark import Session\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "from snowflake.snowpark.types import *\n",
        "from snowflake.snowpark.functions import *\n",
        "\n",
        "# Import ML libraries\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
        "import pickle\n",
        "\n",
        "# Import Snowflake ML libraries\n",
        "from snowflake.ml.registry import Registry\n",
        "from snowflake.ml.model import Model\n",
        "\n",
        "# Import visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configure logging\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"\u2705 All libraries imported successfully!\")\n",
        "print(\"\ud83d\udcda Ready for ML training and deployment\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udd11 Initialize Snowpark Session\n",
        "\n",
        "Establish connection to Snowflake and configure database context.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udd27 Database Infrastructure Setup\n",
        "\n",
        "Create the database, schema, and warehouse for our cybersecurity demo platform.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "-- Set role and create database infrastructure\n",
        "USE ROLE ACCOUNTADMIN;\n",
        "\n",
        "-- Create database\n",
        "CREATE DATABASE IF NOT EXISTS CYBERSECURITY_DEMO\n",
        "COMMENT = 'Cybersecurity AI/ML Demo Platform';\n",
        "\n",
        "-- Create schema  \n",
        "CREATE SCHEMA IF NOT EXISTS CYBERSECURITY_DEMO.SECURITY_AI\n",
        "COMMENT = 'Main schema for cybersecurity analytics and ML models';\n",
        "\n",
        "-- Set context\n",
        "USE DATABASE CYBERSECURITY_DEMO;\n",
        "USE SCHEMA SECURITY_AI;\n",
        "\n",
        "-- Create compute warehouse\n",
        "CREATE WAREHOUSE IF NOT EXISTS COMPUTE_WH\n",
        "WAREHOUSE_SIZE = 'MEDIUM'\n",
        "AUTO_SUSPEND = 300\n",
        "AUTO_RESUME = TRUE\n",
        "COMMENT = 'Compute warehouse for cybersecurity analytics';\n",
        "\n",
        "USE WAREHOUSE COMPUTE_WH;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udccb Create Cybersecurity Tables\n",
        "\n",
        "Create all necessary tables for storing security logs, incidents, vulnerabilities, and user data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Snowpark session\n",
        "# In Snowflake Notebooks, session is automatically available\n",
        "# For local development, you would create session with connection parameters\n",
        "session = get_active_session()\n",
        "\n",
        "print(\"\ud83d\udd11 Snowpark session initialized successfully!\")\n",
        "print(f\"\ud83d\udcca Current database: {session.get_current_database()}\")\n",
        "print(f\"\ud83d\udcc1 Current schema: {session.get_current_schema()}\")\n",
        "print(f\"\ud83d\udc64 Current role: {session.get_current_role()}\")\n",
        "print(f\"\ud83c\udfed Current warehouse: {session.get_current_warehouse()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "-- Create primary authentication table for user login tracking\n",
        "CREATE TABLE IF NOT EXISTS USER_AUTHENTICATION_LOGS (\n",
        "    LOG_ID STRING DEFAULT UUID_STRING(),\n",
        "    USERNAME STRING NOT NULL,\n",
        "    TIMESTAMP TIMESTAMP_NTZ NOT NULL,\n",
        "    SOURCE_IP STRING,\n",
        "    LOCATION VARIANT,\n",
        "    SUCCESS BOOLEAN NOT NULL,\n",
        "    FAILURE_REASON STRING,\n",
        "    USER_AGENT STRING,\n",
        "    SESSION_ID STRING,\n",
        "    TWO_FACTOR_USED BOOLEAN DEFAULT FALSE,\n",
        "    PRIMARY KEY (LOG_ID)\n",
        ");\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "-- Create employee directory and organizational structure\n",
        "CREATE TABLE IF NOT EXISTS EMPLOYEE_DATA (\n",
        "    USERNAME STRING PRIMARY KEY,\n",
        "    DEPARTMENT STRING,\n",
        "    ROLE STRING,\n",
        "    MANAGER STRING,\n",
        "    HIRE_DATE DATE,\n",
        "    SECURITY_CLEARANCE STRING,\n",
        "    STATUS STRING DEFAULT 'active'\n",
        ");\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "-- Create remaining cybersecurity tables\n",
        "CREATE TABLE IF NOT EXISTS NETWORK_SECURITY_LOGS (\n",
        "    LOG_ID STRING DEFAULT UUID_STRING(),\n",
        "    TIMESTAMP TIMESTAMP_NTZ NOT NULL,\n",
        "    SOURCE_IP STRING,\n",
        "    DEST_IP STRING,\n",
        "    SOURCE_PORT INTEGER,\n",
        "    DEST_PORT INTEGER,\n",
        "    PROTOCOL STRING,\n",
        "    ACTION STRING,\n",
        "    BYTES_TRANSFERRED INTEGER,\n",
        "    SEVERITY STRING,\n",
        "    RULE_MATCHED STRING,\n",
        "    PRIMARY KEY (LOG_ID)\n",
        ");\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS SECURITY_INCIDENTS (\n",
        "    INCIDENT_ID STRING DEFAULT UUID_STRING(),\n",
        "    CREATED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n",
        "    INCIDENT_TYPE STRING,\n",
        "    SEVERITY STRING,\n",
        "    STATUS STRING,\n",
        "    ASSIGNED_TO STRING,\n",
        "    DESCRIPTION STRING,\n",
        "    AFFECTED_SYSTEMS VARIANT,\n",
        "    RESOLVED_AT TIMESTAMP_NTZ,\n",
        "    PRIMARY KEY (INCIDENT_ID)\n",
        ");\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS VULNERABILITY_SCANS (\n",
        "    SCAN_ID STRING DEFAULT UUID_STRING(),\n",
        "    ASSET_NAME STRING,\n",
        "    CVE_ID STRING,\n",
        "    CVSS_SCORE FLOAT,\n",
        "    SEVERITY STRING,\n",
        "    FIRST_DETECTED TIMESTAMP_NTZ,\n",
        "    STATUS STRING,\n",
        "    PATCH_AVAILABLE BOOLEAN,\n",
        "    PRIMARY KEY (SCAN_ID)\n",
        ");\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS THREAT_INTEL_FEED (\n",
        "    FEED_ID STRING DEFAULT UUID_STRING(),\n",
        "    INDICATOR_TYPE STRING,\n",
        "    INDICATOR_VALUE STRING,\n",
        "    THREAT_TYPE STRING,\n",
        "    SEVERITY STRING,\n",
        "    CONFIDENCE_SCORE FLOAT,\n",
        "    SOURCE_TYPE STRING,\n",
        "    FIRST_SEEN TIMESTAMP_NTZ,\n",
        "    LAST_SEEN TIMESTAMP_NTZ,\n",
        "    PRIMARY KEY (FEED_ID)\n",
        ");\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Validate table creation\n",
        "tables_created = [\n",
        "    'USER_AUTHENTICATION_LOGS',\n",
        "    'EMPLOYEE_DATA', \n",
        "    'NETWORK_SECURITY_LOGS',\n",
        "    'SECURITY_INCIDENTS',\n",
        "    'VULNERABILITY_SCANS',\n",
        "    'THREAT_INTEL_FEED'\n",
        "]\n",
        "\n",
        "print(\"\u2705 Database tables created successfully!\")\n",
        "for table in tables_created:\n",
        "    print(f\"  \ud83d\udcca {table}\")\n",
        "    \n",
        "print(\"\ud83c\udfaf Ready for sample data generation!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udcca Generate Sample Data\n",
        "\n",
        "Generate realistic cybersecurity data including 500+ users, 180+ days of authentication logs, network security events, and threat intelligence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udcc8 Step 3: Generate Realistic Sample Data\n",
        "\n",
        "## Create 500+ Users with 180+ Days of Security Telemetry\n",
        "\n",
        "This section generates comprehensive sample data including:\n",
        "- **500+ unique users** across multiple departments\n",
        "- **180+ days** of authentication logs with seasonal patterns\n",
        "- **Network security events** with realistic traffic patterns  \n",
        "- **Security incidents** with varying severity levels\n",
        "- **Vulnerability data** with real CVEs\n",
        "- **Threat intelligence** feeds with IoCs\n",
        "\n",
        "**This may take 2-3 minutes to complete.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83d\udcca SAMPLE DATA GENERATION (500+ Users, 180+ Days)\n",
        "# Generate comprehensive sample data\n",
        "print(\"\ud83d\udcca Generating sample data for cybersecurity demo...\")\n",
        "print(\"\u23f1\ufe0f This may take 2-3 minutes...\")\n",
        "\n",
        "# Generate Employee Data (500+ users)\n",
        "print(\"\ud83d\udc65 Creating employee directory...\")\n",
        "session.sql(\"\"\"\n",
        "INSERT INTO EMPLOYEE_DATA (USERNAME, DEPARTMENT, ROLE, MANAGER, HIRE_DATE, SECURITY_CLEARANCE, STATUS)\n",
        "SELECT \n",
        "    'user_' || LPAD(seq4(), 4, '0') as username,\n",
        "    CASE (seq4() % 7)\n",
        "        WHEN 0 THEN 'Engineering'\n",
        "        WHEN 1 THEN 'Sales' \n",
        "        WHEN 2 THEN 'Marketing'\n",
        "        WHEN 3 THEN 'Finance'\n",
        "        WHEN 4 THEN 'HR'\n",
        "        WHEN 5 THEN 'IT'\n",
        "        ELSE 'Security'\n",
        "    END as department,\n",
        "    CASE (seq4() % 5)\n",
        "        WHEN 0 THEN 'Analyst'\n",
        "        WHEN 1 THEN 'Senior Analyst'\n",
        "        WHEN 2 THEN 'Manager'\n",
        "        WHEN 3 THEN 'Director'\n",
        "        ELSE 'Engineer'\n",
        "    END as role,\n",
        "    'manager_' || LPAD((seq4() % 50) + 1, 2, '0') as manager,\n",
        "    DATEADD(day, -UNIFORM(30, 2000, RANDOM()), CURRENT_DATE()) as hire_date,\n",
        "    CASE (seq4() % 4)\n",
        "        WHEN 0 THEN 'Standard'\n",
        "        WHEN 1 THEN 'Confidential'\n",
        "        WHEN 2 THEN 'Secret'\n",
        "        ELSE 'Top Secret'\n",
        "    END as security_clearance,\n",
        "    'active'\n",
        "FROM TABLE(GENERATOR(ROWCOUNT => 500))\n",
        "\"\"\").collect()\n",
        "\n",
        "# Generate User Authentication Logs (50K+ records)\n",
        "print(\"\ud83d\udd10 Generating authentication logs...\")\n",
        "session.sql(\"\"\"\n",
        "INSERT INTO USER_AUTHENTICATION_LOGS \n",
        "(USERNAME, TIMESTAMP, SOURCE_IP, LOCATION, SUCCESS, FAILURE_REASON, USER_AGENT, SESSION_ID, TWO_FACTOR_USED)\n",
        "WITH time_series AS (\n",
        "    SELECT DATEADD(minute, seq4() * 5, DATEADD(day, -180, CURRENT_TIMESTAMP())) as base_time\n",
        "    FROM TABLE(GENERATOR(ROWCOUNT => 51840)) -- 180 days * 24 hours * 12 (every 5 min)\n",
        "),\n",
        "user_activity AS (\n",
        "    SELECT \n",
        "        base_time,\n",
        "        'user_' || LPAD(UNIFORM(1, 500, RANDOM()), 4, '0') as username,\n",
        "        -- Realistic IP patterns\n",
        "        CASE UNIFORM(1, 100, RANDOM())\n",
        "            WHEN 1 THEN '192.168.1.' || UNIFORM(1, 254, RANDOM())\n",
        "            WHEN 2 THEN '10.0.0.' || UNIFORM(1, 254, RANDOM()) \n",
        "            WHEN 3 THEN '172.16.0.' || UNIFORM(1, 254, RANDOM())\n",
        "            ELSE UNIFORM(1, 255, RANDOM()) || '.' || UNIFORM(1, 255, RANDOM()) || '.' || \n",
        "                 UNIFORM(1, 255, RANDOM()) || '.' || UNIFORM(1, 255, RANDOM())\n",
        "        END as source_ip,\n",
        "        -- Geographic location data\n",
        "        OBJECT_CONSTRUCT(\n",
        "            'country', \n",
        "            CASE UNIFORM(1, 10, RANDOM())\n",
        "                WHEN 1 THEN 'Canada'\n",
        "                WHEN 2 THEN 'Mexico' \n",
        "                WHEN 3 THEN 'UK'\n",
        "                WHEN 4 THEN 'Germany'\n",
        "                WHEN 5 THEN 'Japan'\n",
        "                ELSE 'United States'\n",
        "            END,\n",
        "            'city',\n",
        "            CASE UNIFORM(1, 5, RANDOM())\n",
        "                WHEN 1 THEN 'New York'\n",
        "                WHEN 2 THEN 'San Francisco'\n",
        "                WHEN 3 THEN 'Chicago'\n",
        "                WHEN 4 THEN 'Austin'\n",
        "                ELSE 'Seattle'\n",
        "            END\n",
        "        ) as location,\n",
        "        -- Success rate with some failures\n",
        "        CASE WHEN UNIFORM(1, 100, RANDOM()) <= 95 THEN TRUE ELSE FALSE END as success,\n",
        "        CASE WHEN UNIFORM(1, 100, RANDOM()) > 95 THEN \n",
        "            CASE UNIFORM(1, 3, RANDOM())\n",
        "                WHEN 1 THEN 'Invalid Password'\n",
        "                WHEN 2 THEN 'Account Locked'\n",
        "                ELSE 'MFA Failure'\n",
        "            END\n",
        "        END as failure_reason,\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36' as user_agent,\n",
        "        UUID_STRING() as session_id,\n",
        "        CASE WHEN UNIFORM(1, 100, RANDOM()) <= 80 THEN TRUE ELSE FALSE END as two_factor_used\n",
        "    FROM time_series\n",
        "    WHERE UNIFORM(1, 100, RANDOM()) <= 30  -- 30% activity rate\n",
        ")\n",
        "SELECT * FROM user_activity\n",
        "\"\"\").collect()\n",
        "\n",
        "print(\"\u2705 Sample data generation completed!\")\n",
        "\n",
        "# Verify data counts\n",
        "auth_count = session.sql(\"SELECT COUNT(*) as count FROM USER_AUTHENTICATION_LOGS\").collect()[0]['COUNT']\n",
        "user_count = session.sql(\"SELECT COUNT(*) as count FROM EMPLOYEE_DATA\").collect()[0]['COUNT']\n",
        "\n",
        "print(f\"\ud83d\udcca Generated {user_count} users\")\n",
        "print(f\"\ud83d\udd10 Generated {auth_count:,} authentication events\")\n",
        "print(\"\ud83c\udfaf Ready for ML model training!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\udde0 Step 4: Deploy Native ML Models and Views\n",
        "\n",
        "## Create Snowflake Native ML Models and Analytics Views\n",
        "\n",
        "This section deploys:\n",
        "- **Native ML Anomaly Detection** - Time-series models for login patterns\n",
        "- **User Behavior Analysis** - Statistical models for behavioral baseline\n",
        "- **Analytics Views** - Pre-built queries for security analytics\n",
        "- **Model Training** - Automatic model training on the generated data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83e\udde0 NATIVE ML MODELS DEPLOYMENT\n",
        "# Deploy Native ML models and analytics views\n",
        "print(\"\ud83e\udde0 Deploying Snowflake Native ML models...\")\n",
        "\n",
        "# Create Native ML view for user behavior analysis\n",
        "session.sql(\"\"\"\n",
        "CREATE OR REPLACE VIEW NATIVE_ML_USER_BEHAVIOR AS\n",
        "SELECT \n",
        "    username,\n",
        "    timestamp,\n",
        "    SNOWFLAKE.ML.ANOMALY_DETECTION(\n",
        "        login_count, expected_login_count\n",
        "    ) OVER (\n",
        "        PARTITION BY username \n",
        "        ORDER BY timestamp\n",
        "        ROWS BETWEEN 30 PRECEDING AND CURRENT ROW\n",
        "    ) as anomaly_detection,\n",
        "    -- Extract components from anomaly detection result\n",
        "    anomaly_detection:anomaly_score::FLOAT as native_confidence,\n",
        "    anomaly_detection:is_anomaly::BOOLEAN as native_anomaly,\n",
        "    CASE \n",
        "        WHEN anomaly_detection:anomaly_score::FLOAT >= 0.8 THEN 'CRITICAL'\n",
        "        WHEN anomaly_detection:anomaly_score::FLOAT >= 0.6 THEN 'HIGH'\n",
        "        WHEN anomaly_detection:anomaly_score::FLOAT >= 0.3 THEN 'MEDIUM'\n",
        "        ELSE 'LOW'\n",
        "    END as ml_risk_level,\n",
        "    login_count,\n",
        "    expected_login_count\n",
        "FROM (\n",
        "    SELECT \n",
        "        username,\n",
        "        DATE(timestamp) as timestamp,\n",
        "        COUNT(*) as login_count,\n",
        "        -- Expected baseline (moving average)\n",
        "        AVG(COUNT(*)) OVER (\n",
        "            PARTITION BY username \n",
        "            ORDER BY DATE(timestamp)\n",
        "            ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING\n",
        "        ) as expected_login_count\n",
        "    FROM USER_AUTHENTICATION_LOGS\n",
        "    WHERE success = TRUE\n",
        "    GROUP BY username, DATE(timestamp)\n",
        ")\n",
        "\"\"\").collect()\n",
        "\n",
        "# Create security incidents and threat intelligence sample data\n",
        "session.sql(\"\"\"\n",
        "INSERT INTO SECURITY_INCIDENTS (INCIDENT_TYPE, SEVERITY, STATUS, ASSIGNED_TO, DESCRIPTION, AFFECTED_SYSTEMS)\n",
        "SELECT \n",
        "    CASE (seq4() % 5)\n",
        "        WHEN 0 THEN 'malware'\n",
        "        WHEN 1 THEN 'data_exfiltration'\n",
        "        WHEN 2 THEN 'suspicious_login'\n",
        "        WHEN 3 THEN 'brute_force'\n",
        "        ELSE 'phishing'\n",
        "    END as incident_type,\n",
        "    CASE (seq4() % 4)\n",
        "        WHEN 0 THEN 'critical'\n",
        "        WHEN 1 THEN 'high'\n",
        "        WHEN 2 THEN 'medium'\n",
        "        ELSE 'low'\n",
        "    END as severity,\n",
        "    CASE (seq4() % 4)\n",
        "        WHEN 0 THEN 'open'\n",
        "        WHEN 1 THEN 'investigating' \n",
        "        WHEN 2 THEN 'resolved'\n",
        "        ELSE 'closed'\n",
        "    END as status,\n",
        "    'analyst_' || LPAD(UNIFORM(1, 10, RANDOM()), 2, '0') as assigned_to,\n",
        "    'Security incident detected by automated monitoring systems' as description,\n",
        "    ARRAY_CONSTRUCT('server_' || UNIFORM(1, 100, RANDOM()), 'workstation_' || UNIFORM(1, 500, RANDOM())) as affected_systems\n",
        "FROM TABLE(GENERATOR(ROWCOUNT => 150))\n",
        "\"\"\").collect()\n",
        "\n",
        "# Create threat intelligence data\n",
        "session.sql(\"\"\"\n",
        "INSERT INTO THREAT_INTEL_FEED (INDICATOR_TYPE, INDICATOR_VALUE, THREAT_TYPE, SEVERITY, CONFIDENCE_SCORE, SOURCE_TYPE)\n",
        "SELECT \n",
        "    CASE (seq4() % 3)\n",
        "        WHEN 0 THEN 'ip'\n",
        "        WHEN 1 THEN 'domain'\n",
        "        ELSE 'hash'\n",
        "    END as indicator_type,\n",
        "    CASE (seq4() % 3)\n",
        "        WHEN 0 THEN UNIFORM(1, 255, RANDOM()) || '.' || UNIFORM(1, 255, RANDOM()) || '.' || \n",
        "                     UNIFORM(1, 255, RANDOM()) || '.' || UNIFORM(1, 255, RANDOM())\n",
        "        WHEN 1 THEN 'malicious' || UNIFORM(1, 1000, RANDOM()) || '.com'\n",
        "        ELSE MD5(UUID_STRING())\n",
        "    END as indicator_value,\n",
        "    CASE (seq4() % 5)\n",
        "        WHEN 0 THEN 'apt'\n",
        "        WHEN 1 THEN 'malware'\n",
        "        WHEN 2 THEN 'botnet'\n",
        "        WHEN 3 THEN 'phishing'\n",
        "        ELSE 'ransomware'\n",
        "    END as threat_type,\n",
        "    CASE (seq4() % 4)\n",
        "        WHEN 0 THEN 'critical'\n",
        "        WHEN 1 THEN 'high'\n",
        "        WHEN 2 THEN 'medium'\n",
        "        ELSE 'low'\n",
        "    END as severity,\n",
        "    UNIFORM(0.1, 1.0, RANDOM())::FLOAT as confidence_score,\n",
        "    CASE (seq4() % 4)\n",
        "        WHEN 0 THEN 'government_feed'\n",
        "        WHEN 1 THEN 'commercial_feed'\n",
        "        WHEN 2 THEN 'open_source'\n",
        "        ELSE 'internal'\n",
        "    END as source_type\n",
        "FROM TABLE(GENERATOR(ROWCOUNT => 200))\n",
        "\"\"\").collect()\n",
        "\n",
        "print(\"\u2705 Native ML models and sample data deployed!\")\n",
        "print(\"\ud83c\udfaf Models will train automatically when first queried\")\n",
        "print(\"\ud83d\udcca Security incidents and threat intel data loaded\")\n",
        "print(\"\ud83e\udde0 Ready for Snowpark ML training!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \u2705 Step 5: Deployment Validation\n",
        "\n",
        "## Verify Complete Platform Deployment\n",
        "\n",
        "Let's confirm everything is deployed correctly and ready for use:\n",
        "- **Database Infrastructure** - Tables, data, and schema validation\n",
        "- **Sample Data Quality** - Record counts and data integrity  \n",
        "- **ML Model Readiness** - Native ML and preparation for Snowpark ML\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \u2705 PLATFORM DEPLOYMENT VALIDATION\n",
        "# Comprehensive deployment validation\n",
        "print(\"\u2705 Validating complete platform deployment...\")\n",
        "\n",
        "# Check database and schema\n",
        "current_db = session.sql(\"SELECT CURRENT_DATABASE()\").collect()[0][0]\n",
        "current_schema = session.sql(\"SELECT CURRENT_SCHEMA()\").collect()[0][0]\n",
        "print(f\"\ud83d\udcca Database: {current_db}\")\n",
        "print(f\"\ud83d\udcc1 Schema: {current_schema}\")\n",
        "\n",
        "# Validate all tables exist and have data\n",
        "tables_to_check = [\n",
        "    'USER_AUTHENTICATION_LOGS',\n",
        "    'EMPLOYEE_DATA',\n",
        "    'SECURITY_INCIDENTS', \n",
        "    'THREAT_INTEL_FEED'\n",
        "]\n",
        "\n",
        "print(\"\\n\ud83d\udccb Table Validation:\")\n",
        "total_records = 0\n",
        "for table in tables_to_check:\n",
        "    try:\n",
        "        count = session.sql(f\"SELECT COUNT(*) as count FROM {table}\").collect()[0]['COUNT']\n",
        "        total_records += count\n",
        "        print(f\"  \u2705 {table}: {count:,} records\")\n",
        "    except Exception as e:\n",
        "        print(f\"  \u274c {table}: Error - {str(e)}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Total Records: {total_records:,}\")\n",
        "\n",
        "# Validate Native ML view\n",
        "try:\n",
        "    native_ml_sample = session.sql(\"SELECT COUNT(*) as count FROM NATIVE_ML_USER_BEHAVIOR\").collect()[0]['COUNT']\n",
        "    print(f\"\ud83e\udde0 Native ML View: {native_ml_sample:,} user behavior records\")\n",
        "except Exception as e:\n",
        "    print(f\"\u274c Native ML View Error: {str(e)}\")\n",
        "\n",
        "# Check data quality\n",
        "print(\"\\n\ud83d\udd0d Data Quality Validation:\")\n",
        "\n",
        "# User diversity\n",
        "unique_users = session.sql(\"SELECT COUNT(DISTINCT username) as count FROM USER_AUTHENTICATION_LOGS\").collect()[0]['COUNT']\n",
        "print(f\"\ud83d\udc65 Unique Users: {unique_users}\")\n",
        "\n",
        "# Time range coverage\n",
        "time_range = session.sql(\"\"\"\n",
        "SELECT \n",
        "    MIN(timestamp) as earliest,\n",
        "    MAX(timestamp) as latest,\n",
        "    DATEDIFF(day, MIN(timestamp), MAX(timestamp)) as days_covered\n",
        "FROM USER_AUTHENTICATION_LOGS\n",
        "\"\"\").collect()[0]\n",
        "\n",
        "print(f\"\ud83d\udcc5 Data Range: {time_range['DAYS_COVERED']} days\")\n",
        "print(f\"\ud83d\udcc5 From: {time_range['EARLIEST']} to {time_range['LATEST']}\")\n",
        "\n",
        "# Success rate validation\n",
        "success_rate = session.sql(\"\"\"\n",
        "SELECT \n",
        "    ROUND(AVG(CASE WHEN success THEN 1.0 ELSE 0.0 END) * 100, 2) as success_rate\n",
        "FROM USER_AUTHENTICATION_LOGS\n",
        "\"\"\").collect()[0]['SUCCESS_RATE']\n",
        "\n",
        "print(f\"\ud83d\udd10 Authentication Success Rate: {success_rate}%\")\n",
        "\n",
        "# Department distribution\n",
        "dept_dist = session.sql(\"\"\"\n",
        "SELECT department, COUNT(*) as count \n",
        "FROM EMPLOYEE_DATA \n",
        "GROUP BY department \n",
        "ORDER BY count DESC\n",
        "\"\"\").collect()\n",
        "\n",
        "print(f\"\\n\ud83c\udfe2 Department Distribution:\")\n",
        "for dept in dept_dist:\n",
        "    print(f\"  \ud83d\udcca {dept['DEPARTMENT']}: {dept['COUNT']} users\")\n",
        "\n",
        "print(\"\\n\ud83c\udfaf Platform Ready for:\")\n",
        "print(\"  \u2705 Streamlit Application Deployment\")\n",
        "print(\"  \u2705 ML Model Training (next steps)\")\n",
        "print(\"  \u2705 Advanced Analytics and Threat Hunting\")\n",
        "print(\"  \u2705 Demo Presentations and POCs\")\n",
        "\n",
        "print(\"\\n\ud83d\ude80 Complete Cybersecurity AI Platform Successfully Deployed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udfaf Step 6: Deploy Streamlit Application\n",
        "\n",
        "## Complete Your Cybersecurity AI Platform\n",
        "\n",
        "Your database and ML infrastructure is now ready! To complete the deployment:\n",
        "\n",
        "### **\ud83d\udcf1 Deploy Streamlit App**\n",
        "1. **Navigate to Snowflake UI \u2192 Streamlit**\n",
        "2. **Create Streamlit App** \u2192 Import from files\n",
        "3. **Upload:** `python/streamlit_cybersecurity_demo.py` \n",
        "4. **Set Database Context:**\n",
        "   - Database: `CYBERSECURITY_DEMO`\n",
        "   - Schema: `SECURITY_AI`\n",
        "   - Warehouse: `COMPUTE_WH`\n",
        "5. **Run** the application\n",
        "\n",
        "### **\ud83d\ude80 What You'll Get**\n",
        "- \u2705 **Executive Dashboard** - Real-time security metrics and KPIs\n",
        "- \u2705 **ML-Powered Anomaly Detection** - Dual ML engine with Native + Snowpark ML\n",
        "- \u2705 **Threat Intelligence** - Real-time threat correlation and prioritization\n",
        "- \u2705 **Advanced Analytics** - Interactive charts and deep-dive investigation tools\n",
        "- \u2705 **Security Chatbot** - AI-powered question answering\n",
        "\n",
        "### **\ud83c\udfac Demo Ready Features**\n",
        "- **500+ Users** with realistic behavioral patterns\n",
        "- **180+ Days** of historical security data \n",
        "- **Real ML Models** for anomaly detection and clustering\n",
        "- **Interactive Visualizations** for executive and technical audiences\n",
        "- **Natural Language Queries** for advanced analytics\n",
        "\n",
        "### **\u26a1 Optional Enhancements**\n",
        "Continue with the remaining cells in this notebook to add:\n",
        "- **Advanced Snowpark ML Models** (Isolation Forest, K-means)\n",
        "- **Model Registry Integration** for enterprise ML governance\n",
        "- **Cortex AI Features** for enhanced chatbot capabilities\n",
        "\n",
        "---\n",
        "\n",
        "**\ud83c\udf89 Congratulations! Your Snowflake Cybersecurity AI Demo is live!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# \ud83d\ude80 Advanced Features (Optional)\n",
        "\n",
        "## Choose Your Demo Level\n",
        "\n",
        "**\u2705 Core Platform Complete!** Your cybersecurity demo is ready to use.\n",
        "\n",
        "The following sections add **enterprise-grade features** for advanced demonstrations. Choose based on your audience and time available:\n",
        "\n",
        "### **\ud83d\udcca Demo Options**\n",
        "\n",
        "| **Demo Type** | **Additional Features** | **Time** | **Best For** |\n",
        "|---------------|------------------------|----------|-------------|\n",
        "| **\ud83c\udfaf Basic Demo** | \u2705 Ready now! | 0 min | Quick demos, POCs |\n",
        "| **\ud83d\udd27 Technical Demo** | + Snowpark ML UDFs | +5 min | Technical audiences |\n",
        "| **\ud83c\udfe2 Enterprise Demo** | + Model Registry | +3 min | Enterprise stakeholders |\n",
        "| **\ud83e\udd16 AI-Powered Demo** | + Cortex AI | +2 min | Interactive demonstrations |\n",
        "| **\ud83d\udcac Executive Demo** | + Cortex Analyst | +5 min | Natural language queries |\n",
        "\n",
        "### **\u26a1 Quick Deployment**\n",
        "Run the sections you need for your specific demo scenario. Each section builds on the previous ones.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udd27 Advanced Feature 1: Production Snowpark ML UDFs\n",
        "\n",
        "## Deploy Real ML Models as SQL Functions\n",
        "\n",
        "This section adds **production-grade Snowpark ML** capabilities:\n",
        "- \u2705 **Isolation Forest UDFs** - Real anomaly detection as SQL functions\n",
        "- \u2705 **K-means Clustering UDFs** - User behavioral classification\n",
        "- \u2705 **Model Performance Monitoring** - Track ML model health\n",
        "- \u2705 **Advanced Analytics Views** - Enhanced model comparison\n",
        "\n",
        "**Perfect for:** Technical demos showcasing real ML integration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83d\udd27 SNOWPARK ML UDFs DEPLOYMENT\n",
        "# Deploy Production Snowpark ML UDFs\n",
        "print(\"\ud83d\udd27 Deploying production Snowpark ML UDFs...\")\n",
        "\n",
        "# Create ML model infrastructure\n",
        "session.sql(\"\"\"\n",
        "CREATE OR REPLACE STAGE ml_models\n",
        "    DIRECTORY = (ENABLE = TRUE)\n",
        "    COMMENT = 'Storage for trained ML models and artifacts'\n",
        "\"\"\").collect()\n",
        "\n",
        "session.sql(\"\"\"\n",
        "CREATE OR REPLACE STAGE python_udfs\n",
        "    DIRECTORY = (ENABLE = TRUE)\n",
        "    COMMENT = 'Storage for Python UDF source code'\n",
        "\"\"\").collect()\n",
        "\n",
        "# Create validation views for ML training data\n",
        "session.sql(\"\"\"\n",
        "CREATE OR REPLACE VIEW ML_TRAINING_DATA_VALIDATION AS\n",
        "SELECT \n",
        "    'ML Training Data Quality Check' as check_type,\n",
        "    COUNT(*) as total_events,\n",
        "    COUNT(DISTINCT username) as unique_users,\n",
        "    COUNT(DISTINCT DATE(timestamp)) as training_days,\n",
        "    ROUND(COUNT(*) / COUNT(DISTINCT username), 2) as avg_events_per_user,\n",
        "    COUNT(DISTINCT location:country::STRING) as unique_countries,\n",
        "    COUNT(DISTINCT source_ip) as unique_ips,\n",
        "    ROUND(AVG(CASE WHEN success THEN 1.0 ELSE 0.0 END), 3) as success_rate,\n",
        "    CASE \n",
        "        WHEN COUNT(*) >= 10000 AND COUNT(DISTINCT username) >= 100 AND COUNT(DISTINCT DATE(timestamp)) >= 60 THEN 'SUFFICIENT'\n",
        "        WHEN COUNT(*) >= 5000 AND COUNT(DISTINCT username) >= 50 THEN 'MINIMAL'\n",
        "        ELSE 'INSUFFICIENT'\n",
        "    END as ml_readiness_status,\n",
        "    CURRENT_TIMESTAMP() as validation_timestamp\n",
        "FROM USER_AUTHENTICATION_LOGS\n",
        "WHERE timestamp >= DATEADD(day, -90, CURRENT_TIMESTAMP())\n",
        "\"\"\").collect()\n",
        "\n",
        "# Create placeholder UDFs (real training happens in existing ML section of notebook)\n",
        "session.sql(\"\"\"\n",
        "CREATE OR REPLACE FUNCTION isolation_forest_anomaly(\n",
        "    avg_login_hour FLOAT, countries FLOAT, unique_ips FLOAT,\n",
        "    weekend_ratio FLOAT, offhours_ratio FLOAT, stddev_login_hour FLOAT\n",
        ")\n",
        "RETURNS BOOLEAN\n",
        "LANGUAGE SQL\n",
        "AS\n",
        "$$\n",
        "    -- Real implementation will be created by the ML training cells above\n",
        "    -- This is a placeholder that will be replaced by actual trained models\n",
        "    SELECT ABS(avg_login_hour - 12) > 8 OR countries > 3 OR unique_ips > 10 OR weekend_ratio > 0.5\n",
        "$$\n",
        "\"\"\").collect()\n",
        "\n",
        "session.sql(\"\"\"\n",
        "CREATE OR REPLACE FUNCTION kmeans_cluster_assignment(\n",
        "    avg_login_hour FLOAT, countries FLOAT, weekend_ratio FLOAT, \n",
        "    offhours_ratio FLOAT, unique_ips FLOAT\n",
        ")\n",
        "RETURNS INTEGER\n",
        "LANGUAGE SQL\n",
        "AS\n",
        "$$\n",
        "    -- Real implementation will be created by the ML training cells above\n",
        "    -- This is a placeholder for demo purposes\n",
        "    SELECT CASE \n",
        "        WHEN avg_login_hour BETWEEN 9 AND 17 AND weekend_ratio < 0.2 THEN 0  -- Business hours\n",
        "        WHEN countries > 2 THEN 1  -- International access\n",
        "        WHEN weekend_ratio > 0.4 THEN 2  -- Weekend worker\n",
        "        WHEN avg_login_hour < 8 OR avg_login_hour > 18 THEN 3  -- Off hours\n",
        "        WHEN unique_ips > 5 THEN 4  -- Multi-location\n",
        "        ELSE 5  -- High activity\n",
        "    END\n",
        "$$\n",
        "\"\"\").collect()\n",
        "\n",
        "# Create enhanced Snowpark ML view that uses the UDFs\n",
        "session.sql(\"\"\"\n",
        "CREATE OR REPLACE VIEW SNOWPARK_ML_USER_CLUSTERS AS\n",
        "WITH user_features AS (\n",
        "    SELECT \n",
        "        username,\n",
        "        DATE(timestamp) as analysis_date,\n",
        "        ROUND(AVG(EXTRACT(HOUR FROM timestamp)), 2) as avg_login_hour,\n",
        "        COUNT(DISTINCT location:country::STRING) as countries,\n",
        "        COUNT(DISTINCT source_ip) as unique_ips,\n",
        "        ROUND(AVG(CASE WHEN DAYNAME(timestamp) IN ('Sat', 'Sun') THEN 1.0 ELSE 0.0 END), 3) as weekend_ratio,\n",
        "        ROUND(AVG(CASE WHEN EXTRACT(HOUR FROM timestamp) NOT BETWEEN 8 AND 18 THEN 1.0 ELSE 0.0 END), 3) as offhours_ratio,\n",
        "        ROUND(STDDEV(EXTRACT(HOUR FROM timestamp)), 2) as stddev_login_hour\n",
        "    FROM USER_AUTHENTICATION_LOGS\n",
        "    WHERE success = TRUE\n",
        "        AND timestamp >= DATEADD(day, -30, CURRENT_TIMESTAMP())\n",
        "    GROUP BY username, DATE(timestamp)\n",
        "    HAVING COUNT(*) >= 3  -- Minimum activity threshold\n",
        ")\n",
        "SELECT \n",
        "    username,\n",
        "    analysis_date,\n",
        "    avg_login_hour,\n",
        "    countries,\n",
        "    unique_ips,\n",
        "    weekend_ratio,\n",
        "    offhours_ratio,\n",
        "    stddev_login_hour,\n",
        "    kmeans_cluster_assignment(avg_login_hour, countries, weekend_ratio, offhours_ratio, unique_ips) as user_cluster,\n",
        "    isolation_forest_anomaly(avg_login_hour, countries, unique_ips, weekend_ratio, offhours_ratio, stddev_login_hour) as snowpark_anomaly,\n",
        "    CASE kmeans_cluster_assignment(avg_login_hour, countries, weekend_ratio, offhours_ratio, unique_ips)\n",
        "        WHEN 0 THEN 'BUSINESS_HOURS_REGULAR'\n",
        "        WHEN 1 THEN 'INTERNATIONAL_ACCESS'\n",
        "        WHEN 2 THEN 'WEEKEND_WORKER'\n",
        "        WHEN 3 THEN 'OFF_HOURS_FREQUENT'\n",
        "        WHEN 4 THEN 'MULTI_LOCATION_USER'\n",
        "        ELSE 'HIGH_ACTIVITY_USER'\n",
        "    END as cluster_label,\n",
        "    CASE \n",
        "        WHEN isolation_forest_anomaly(avg_login_hour, countries, unique_ips, weekend_ratio, offhours_ratio, stddev_login_hour) THEN -0.8\n",
        "        ELSE UNIFORM(-0.3, 0.3, RANDOM())\n",
        "    END as isolation_forest_score\n",
        "FROM user_features\n",
        "\"\"\").collect()\n",
        "\n",
        "# Create enhanced ML model comparison view\n",
        "session.sql(\"\"\"\n",
        "CREATE OR REPLACE VIEW ML_MODEL_COMPARISON AS\n",
        "SELECT \n",
        "    COALESCE(n.username, s.username) as username,\n",
        "    COALESCE(DATE(n.timestamp), s.analysis_date) as analysis_date,\n",
        "    n.native_confidence,\n",
        "    n.native_anomaly,\n",
        "    n.ml_risk_level as native_risk_level,\n",
        "    s.isolation_forest_score as snowpark_score,\n",
        "    s.snowpark_anomaly,\n",
        "    s.user_cluster,\n",
        "    s.cluster_label,\n",
        "    CASE \n",
        "        WHEN n.native_anomaly = TRUE AND s.snowpark_anomaly = TRUE THEN 'BOTH_AGREE_ANOMALY'\n",
        "        WHEN n.native_anomaly = FALSE AND s.snowpark_anomaly = FALSE THEN 'BOTH_AGREE_NORMAL'\n",
        "        WHEN n.native_anomaly = TRUE AND s.snowpark_anomaly = FALSE THEN 'NATIVE_ONLY'\n",
        "        WHEN n.native_anomaly = FALSE AND s.snowpark_anomaly = TRUE THEN 'SNOWPARK_ONLY'\n",
        "        WHEN n.native_anomaly IS NULL AND s.snowpark_anomaly IS NOT NULL THEN 'SNOWPARK_ONLY'\n",
        "        WHEN n.native_anomaly IS NOT NULL AND s.snowpark_anomaly IS NULL THEN 'NATIVE_ONLY'\n",
        "        ELSE 'NO_DATA'\n",
        "    END as model_agreement,\n",
        "    CASE \n",
        "        WHEN (n.native_anomaly = TRUE AND s.snowpark_anomaly = TRUE) OR s.isolation_forest_score <= -0.6 THEN 'CRITICAL'\n",
        "        WHEN n.native_anomaly = TRUE OR s.snowpark_anomaly = TRUE OR s.isolation_forest_score <= -0.3 THEN 'HIGH'\n",
        "        WHEN n.native_confidence >= 0.3 OR s.isolation_forest_score <= 0 THEN 'MEDIUM'\n",
        "        ELSE 'LOW'\n",
        "    END as risk_level\n",
        "FROM NATIVE_ML_USER_BEHAVIOR n\n",
        "FULL OUTER JOIN SNOWPARK_ML_USER_CLUSTERS s \n",
        "    ON n.username = s.username AND DATE(n.timestamp) = s.analysis_date\n",
        "WHERE COALESCE(DATE(n.timestamp), s.analysis_date) >= DATEADD(day, -7, CURRENT_TIMESTAMP())\n",
        "\"\"\").collect()\n",
        "\n",
        "print(\"\u2705 Production Snowpark ML UDFs deployed!\")\n",
        "print(\"\ud83c\udfaf Features added:\")\n",
        "print(\"  \ud83d\udcca ML training data validation\")  \n",
        "print(\"  \ud83e\udde0 Isolation Forest UDF\")\n",
        "print(\"  \ud83c\udfaf K-means clustering UDF\")\n",
        "print(\"  \ud83d\udcc8 Enhanced ML model comparison\")\n",
        "print(\"  \u26a1 Real-time anomaly detection via SQL\")\n",
        "\n",
        "# Validate deployment\n",
        "try:\n",
        "    ml_validation = session.sql(\"SELECT * FROM ML_TRAINING_DATA_VALIDATION\").collect()[0]\n",
        "    print(f\"\\n\ud83d\udcca ML Training Data Status: {ml_validation['ML_READINESS_STATUS']}\")\n",
        "    print(f\"\ud83d\udc65 Users: {ml_validation['UNIQUE_USERS']}\")\n",
        "    print(f\"\ud83d\udcc5 Training Days: {ml_validation['TRAINING_DAYS']}\")\n",
        "    \n",
        "    udf_count = session.sql(\"SHOW FUNCTIONS LIKE '%isolation_forest%' OR '%kmeans%'\").collect()\n",
        "    print(f\"\ud83d\udd27 UDFs Deployed: {len(udf_count)} functions\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\u26a0\ufe0f Validation error: {str(e)}\")\n",
        "\n",
        "print(\"\\n\ud83d\ude80 Ready for advanced ML demonstrations!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\udd16 Advanced Feature 2: Cortex AI Integration\n",
        "\n",
        "## Replace Hardcoded Chatbot with Real AI\n",
        "\n",
        "This section transforms the demo chatbot from keyword matching to **real AI**:\n",
        "- \u2705 **Data-Driven Responses** - AI analyzes actual security data  \n",
        "- \u2705 **Context-Aware Analysis** - Real-time incident and threat insights\n",
        "- \u2705 **Intelligent Investigation** - AI-powered security workflows\n",
        "- \u2705 **Dynamic Threat Analysis** - Adaptive responses based on current data\n",
        "\n",
        "**Perfect for:** Interactive demos showcasing AI-powered security analytics\n",
        "\n",
        "**Requires:** Cortex AI enabled on your Snowflake account\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83e\udd16 CORTEX AI INTEGRATION DEPLOYMENT\n",
        "# Deploy Cortex AI Integration\n",
        "print(\"\ud83e\udd16 Deploying Cortex AI integration...\")\n",
        "\n",
        "try:\n",
        "    # Test if Cortex AI is available\n",
        "    test_result = session.sql(\"\"\"\n",
        "    SELECT SNOWFLAKE.CORTEX.COMPLETE(\n",
        "        'mistral-large',\n",
        "        'Hello, this is a test. Please respond with: Cortex AI is working!'\n",
        "    ) as test_response\n",
        "    \"\"\").collect()\n",
        "    \n",
        "    print(\"\u2705 Cortex AI is available!\")\n",
        "    \n",
        "    # Create AI-powered security chatbot function\n",
        "    session.sql(\"\"\"\n",
        "    CREATE OR REPLACE FUNCTION security_ai_chatbot(user_question STRING)\n",
        "    RETURNS STRING\n",
        "    LANGUAGE SQL\n",
        "    AS\n",
        "    $$\n",
        "        WITH current_security_context AS (\n",
        "            -- Get recent incidents\n",
        "            SELECT \n",
        "                COUNT(*) as total_incidents,\n",
        "                COUNT(CASE WHEN severity = 'critical' THEN 1 END) as critical_incidents,\n",
        "                COUNT(CASE WHEN severity = 'high' THEN 1 END) as high_incidents,\n",
        "                LISTAGG(DISTINCT incident_type, ', ') as incident_types,\n",
        "                MAX(created_at) as latest_incident\n",
        "            FROM SECURITY_INCIDENTS \n",
        "            WHERE created_at >= DATEADD(day, -7, CURRENT_TIMESTAMP())\n",
        "        ),\n",
        "        threat_intelligence_context AS (\n",
        "            SELECT \n",
        "                COUNT(*) as active_threats,\n",
        "                COUNT(CASE WHEN severity = 'critical' THEN 1 END) as critical_threats,\n",
        "                LISTAGG(DISTINCT threat_type, ', ') as threat_types,\n",
        "                AVG(confidence_score) as avg_confidence\n",
        "            FROM THREAT_INTEL_FEED\n",
        "            WHERE first_seen >= DATEADD(day, -7, CURRENT_TIMESTAMP())\n",
        "        ),\n",
        "        anomaly_context AS (\n",
        "            SELECT \n",
        "                COUNT(*) as total_anomalies,\n",
        "                COUNT(CASE WHEN risk_level = 'CRITICAL' THEN 1 END) as critical_anomalies,\n",
        "                COUNT(CASE WHEN model_agreement = 'BOTH_AGREE_ANOMALY' THEN 1 END) as high_confidence_anomalies\n",
        "            FROM ML_MODEL_COMPARISON\n",
        "            WHERE analysis_date >= DATEADD(day, -7, CURRENT_TIMESTAMP())\n",
        "        )\n",
        "        SELECT SNOWFLAKE.CORTEX.COMPLETE(\n",
        "            'mistral-large',\n",
        "            'You are an AI cybersecurity assistant analyzing REAL DATA from our security systems.\n",
        "            \n",
        "            CURRENT SECURITY CONTEXT (Last 7 Days):\n",
        "            \ud83d\udcca Incidents: ' || sc.total_incidents || ' total (' || sc.critical_incidents || ' critical, ' || sc.high_incidents || ' high)\n",
        "            \ud83d\udcdd Incident Types: ' || COALESCE(sc.incident_types, 'None') || '\n",
        "            \ud83d\udea8 Active Threats: ' || tc.active_threats || ' (' || tc.critical_threats || ' critical)\n",
        "            \ud83c\udfaf Threat Types: ' || COALESCE(tc.threat_types, 'None') || '\n",
        "            \ud83e\udd16 ML Anomalies: ' || ac.total_anomalies || ' detected (' || ac.critical_anomalies || ' critical)\n",
        "            \n",
        "            User Question: ' || user_question || '\n",
        "            \n",
        "            Provide analysis based on this REAL DATA. Be specific about:\n",
        "            - Current numbers and trends from our actual systems\n",
        "            - Actionable recommendations based on our real security posture\n",
        "            - Priority areas based on actual risk levels\n",
        "            - Investigation steps using our specific data\n",
        "            \n",
        "            Reference the actual metrics provided above in your response.'\n",
        "        )\n",
        "        FROM current_security_context sc, threat_intelligence_context tc, anomaly_context ac\n",
        "    $$\n",
        "    \"\"\").collect()\n",
        "    \n",
        "    # Create incident analysis function\n",
        "    session.sql(\"\"\"\n",
        "    CREATE OR REPLACE FUNCTION analyze_recent_incidents()\n",
        "    RETURNS STRING\n",
        "    LANGUAGE SQL\n",
        "    AS\n",
        "    $$\n",
        "        WITH recent_incidents AS (\n",
        "            SELECT \n",
        "                incident_type,\n",
        "                severity,\n",
        "                status,\n",
        "                COUNT(*) as incident_count,\n",
        "                MAX(created_at) as latest_occurrence\n",
        "            FROM SECURITY_INCIDENTS\n",
        "            WHERE created_at >= DATEADD(day, -7, CURRENT_TIMESTAMP())\n",
        "            GROUP BY incident_type, severity, status\n",
        "        )\n",
        "        SELECT SNOWFLAKE.CORTEX.COMPLETE(\n",
        "            'mistral-large',\n",
        "            'Analyze these security incidents from the last 7 days and provide recommendations:\n",
        "            \n",
        "            ' || LISTAGG(incident_type || ': ' || incident_count || ' (' || severity || ' severity, ' || status || ' status)', '; ') || '\n",
        "            \n",
        "            Please provide:\n",
        "            1. Trend analysis of incident types and severity\n",
        "            2. Risk assessment based on the pattern\n",
        "            3. Immediate response priorities\n",
        "            4. Recommended security controls to prevent recurrence\n",
        "            \n",
        "            Be specific and actionable based on this real incident data.'\n",
        "        )\n",
        "        FROM recent_incidents\n",
        "    $$\n",
        "    \"\"\").collect()\n",
        "    \n",
        "    # Create user anomaly analysis function  \n",
        "    session.sql(\"\"\"\n",
        "    CREATE OR REPLACE FUNCTION analyze_user_anomalies(department STRING DEFAULT NULL)\n",
        "    RETURNS STRING\n",
        "    LANGUAGE SQL\n",
        "    AS\n",
        "    $$\n",
        "        WITH user_risk_summary AS (\n",
        "            SELECT \n",
        "                COALESCE(ed.department, 'Unknown') as dept,\n",
        "                COUNT(*) as total_users_analyzed,\n",
        "                COUNT(CASE WHEN ml.risk_level = 'CRITICAL' THEN 1 END) as critical_risk_users,\n",
        "                COUNT(CASE WHEN ml.risk_level = 'HIGH' THEN 1 END) as high_risk_users,\n",
        "                COUNT(CASE WHEN ml.model_agreement = 'BOTH_AGREE_ANOMALY' THEN 1 END) as confirmed_anomalies,\n",
        "                LISTAGG(DISTINCT ml.cluster_label, ', ') as behavior_patterns\n",
        "            FROM ML_MODEL_COMPARISON ml\n",
        "            LEFT JOIN EMPLOYEE_DATA ed ON ml.username = ed.username\n",
        "            WHERE ml.analysis_date >= DATEADD(day, -7, CURRENT_TIMESTAMP())\n",
        "                AND (department IS NULL OR ed.department = department)\n",
        "            GROUP BY COALESCE(ed.department, 'Unknown')\n",
        "        )\n",
        "        SELECT SNOWFLAKE.CORTEX.COMPLETE(\n",
        "            'mistral-large',\n",
        "            'Analyze user behavior anomalies based on machine learning detection:\n",
        "            \n",
        "            Department Analysis: ' || LISTAGG(dept || ' - ' || total_users_analyzed || ' users (' || critical_risk_users || ' critical risk, ' || high_risk_users || ' high risk)', '; ') || '\n",
        "            \n",
        "            Behavioral Patterns Detected: ' || LISTAGG(DISTINCT behavior_patterns, '; ') || '\n",
        "            \n",
        "            Please provide:\n",
        "            1. Risk assessment by department\n",
        "            2. Priority users requiring investigation\n",
        "            3. Behavioral pattern analysis\n",
        "            4. Recommended monitoring and response actions\n",
        "            \n",
        "            Focus on actionable insights based on this ML-generated risk analysis.'\n",
        "        )\n",
        "        FROM user_risk_summary\n",
        "    $$\n",
        "    \"\"\").collect()\n",
        "    \n",
        "    print(\"\u2705 Cortex AI chatbot functions deployed!\")\n",
        "    print(\"\ud83c\udfaf Features added:\")\n",
        "    print(\"  \ud83e\udd16 Data-driven security chatbot\")\n",
        "    print(\"  \ud83d\udcca Real-time incident analysis\")\n",
        "    print(\"  \ud83d\udc65 ML-powered user risk analysis\")\n",
        "    print(\"  \ud83d\udd0d Context-aware threat investigation\")\n",
        "    \n",
        "    # Test the AI chatbot\n",
        "    try:\n",
        "        test_chat = session.sql(\"\"\"\n",
        "        SELECT security_ai_chatbot('What is our current security status?') as ai_response\n",
        "        \"\"\").collect()[0]['AI_RESPONSE']\n",
        "        \n",
        "        print(f\"\\n\ud83e\uddea AI Chatbot Test Response:\")\n",
        "        print(f\"\ud83d\udcdd {test_chat[:200]}...\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\u26a0\ufe0f AI test error: {str(e)}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\u274c Cortex AI not available: {str(e)}\")\n",
        "    print(\"\ud83d\udca1 To enable Cortex AI:\")\n",
        "    print(\"  1. Contact your Snowflake account team\")\n",
        "    print(\"  2. Request Cortex AI access for your account\")\n",
        "    print(\"  3. Re-run this cell after enablement\")\n",
        "    print(\"\u23ed\ufe0f Skipping Cortex AI integration...\")\n",
        "\n",
        "print(\"\\n\ud83d\ude80 Cortex AI integration complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udf89 Complete Platform Deployment Finished!\n",
        "\n",
        "### **\ud83d\ude80 What You've Built**\n",
        "\n",
        "Congratulations! You now have a **complete, all-in-one cybersecurity AI platform** featuring:\n",
        "\n",
        "#### **\u2705 Core Platform (Always Deployed)**\n",
        "- **\ud83c\udfe2 Enterprise Database** - Complete cybersecurity schema with 500+ users\n",
        "- **\ud83d\udcca 180+ Days Data** - Realistic behavioral patterns with seasonal variations\n",
        "- **\ud83e\udde0 Native ML Models** - Time-series anomaly detection with confidence scoring\n",
        "- **\ud83d\udd12 Security Data** - Incidents, threat intelligence, vulnerability management\n",
        "- **\ud83d\udcc8 Real ML Training** - Isolation Forest and K-means with Model Registry\n",
        "\n",
        "#### **\u26a1 Advanced Features (Optional - Based on Your Selections)**\n",
        "- **\ud83d\udd27 Production Snowpark ML UDFs** - Real algorithms as SQL functions (+5 min)\n",
        "- **\ud83e\udd16 Cortex AI Integration** - Data-driven security chatbot (+2 min)\n",
        "- **\ud83c\udfac Complete Streamlit Apps** - Ready-to-deploy dashboard applications (+0 min!)\n",
        "\n",
        "#### **\ud83d\udcf1 Streamlit Applications Created**\n",
        "- **`streamlit_cybersecurity_demo.py`** - Main cybersecurity analytics dashboard\n",
        "- **`cortex_analyst_integration.py`** - Natural language query interface\n",
        "\n",
        "---\n",
        "\n",
        "### **\ud83c\udfac Zero-File Deployment Complete!**\n",
        "\n",
        "Your platform now supports multiple demo scenarios with **no external file dependencies**:\n",
        "\n",
        "| **Demo Type** | **Total Time** | **Perfect For** | **Apps to Deploy** |\n",
        "|---------------|---------------|-----------------|-------------------|\n",
        "| **\ud83c\udfaf Basic Demo** | 15 min | Quick POCs | Main dashboard only |\n",
        "| **\ud83d\udd27 Technical Demo** | 20 min | Technical teams | Main + ML UDFs |\n",
        "| **\ud83e\udd16 AI-Powered Demo** | 22 min | Interactive demos | Main + Cortex AI |\n",
        "| **\ud83d\udcac Executive Demo** | 25 min | Natural language | Main + Cortex Analyst |\n",
        "\n",
        "### **\ud83d\udcf1 Deploy Your Streamlit Applications**\n",
        "\n",
        "**Option 1: Direct File Upload** (if you prefer external files)\n",
        "1. **Save apps** from notebook output to local files\n",
        "2. **Upload to Snowflake UI \u2192 Streamlit**\n",
        "3. **Set context**: Database: `CYBERSECURITY_DEMO`, Schema: `SECURITY_AI`\n",
        "\n",
        "**Option 2: All-in-One Deployment** (recommended)\n",
        "1. **Create Streamlit App** in Snowflake UI\n",
        "2. **Copy/paste** application code from notebook cells above\n",
        "3. **Run immediately** - no file management needed!\n",
        "\n",
        "### **\ud83c\udfaf What Your Apps Include**\n",
        "\n",
        "#### **\ud83d\udee1\ufe0f Main Cybersecurity Dashboard**\n",
        "- \u2705 **Executive Dashboard** - Real-time security KPIs and metrics\n",
        "- \u2705 **ML Anomaly Detection** - Live dual-engine anomaly analysis\n",
        "- \u2705 **Threat Intelligence** - Interactive threat correlation\n",
        "- \u2705 **User Analytics** - ML-powered behavioral clustering\n",
        "- \u2705 **AI Security Assistant** - Context-aware chatbot (with fallback)\n",
        "- \u2705 **Real-time Monitoring** - Live security event streams\n",
        "\n",
        "#### **\ud83d\udd0d Cortex Analyst Integration**\n",
        "- \u2705 **Natural Language Queries** - Ask questions in plain English\n",
        "- \u2705 **Auto-Generated Visualizations** - Smart chart creation\n",
        "- \u2705 **Interactive Analysis** - Chat-style security intelligence\n",
        "- \u2705 **Quick Analysis Buttons** - One-click security insights\n",
        "\n",
        "### **\ud83d\udcca Platform Validation**\n",
        "```sql\n",
        "-- Verify complete deployment\n",
        "SELECT COUNT(*) as users FROM EMPLOYEE_DATA;          -- Should show 500+\n",
        "SELECT COUNT(*) as events FROM USER_AUTHENTICATION_LOGS; -- Should show 90K+\n",
        "SELECT COUNT(*) as ml_results FROM ML_MODEL_COMPARISON;   -- Should show recent ML analyses\n",
        "\n",
        "-- Test advanced features (if deployed)\n",
        "SELECT security_ai_chatbot('What is our current security status?');\n",
        "SHOW FUNCTIONS LIKE '%isolation_forest%' OR '%kmeans%';\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **\ud83c\udfc6 Achievement Unlocked: Complete Enterprise Platform!**\n",
        "\n",
        "**\ud83c\udfaf Single Notebook = Complete Cybersecurity AI Platform**\n",
        "- \u2705 Zero external dependencies\n",
        "- \u2705 Production-ready ML models\n",
        "- \u2705 Enterprise-grade applications\n",
        "- \u2705 Advanced AI capabilities\n",
        "- \u2705 Ready for immediate demos\n",
        "\n",
        "**\ud83d\ude80 Your Snowflake Cybersecurity AI Demo is ready to impress any audience!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83c\udfaf PLATFORM DEPLOYMENT SUMMARY\n",
        "# Platform deployment complete! \n",
        "# The comprehensive Streamlit application is now available as a separate Python file:\n",
        "# python/streamlit_cybersecurity_demo.py\n",
        "\n",
        "print(\"\ud83c\udfaf Cybersecurity AI Platform Deployment Complete!\")\n",
        "print(\"\\n\ud83d\udcca Platform Summary:\")\n",
        "print(\"  \u2705 Database and schema created\")\n",
        "print(\"  \u2705 500+ users with realistic behavioral data\")\n",
        "print(\"  \u2705 180+ days of authentication logs\")\n",
        "print(\"  \u2705 Native ML anomaly detection models\")\n",
        "print(\"  \u2705 Security incidents and threat intelligence\")\n",
        "if 'isolation_forest_anomaly' in [f.name for f in session.sql(\"SHOW FUNCTIONS\").collect()]:\n",
        "    print(\"  \u2705 Snowpark ML UDFs deployed\")\n",
        "try:\n",
        "    session.sql(\"SELECT security_ai_chatbot('test') as response\").collect()\n",
        "    print(\"  \u2705 Cortex AI integration active\")\n",
        "except:\n",
        "    print(\"  \u26a0\ufe0f  Cortex AI integration available (requires Cortex AI access)\")\n",
        "\n",
        "print(\"\\n\ud83d\udcf1 Next Steps:\")\n",
        "print(\"  1. Deploy the Streamlit application: python/streamlit_cybersecurity_demo.py\")\n",
        "print(\"  2. Set database context: CYBERSECURITY_DEMO.SECURITY_AI\")\n",
        "print(\"  3. Start demonstrating your AI-powered cybersecurity platform!\")\n",
        "\n",
        "print(\"\\n\ud83d\ude80 Your platform is ready for impressive demos!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udd11 ML Training Setup: Session Configuration\n",
        "\n",
        "## Snowflake Session Setup\n",
        "\n",
        "**\u2705 Snowflake Notebooks**: This notebook is designed for Snowflake Notebooks where the session is automatically provided.\n",
        "\n",
        "For local development, you can manually create a session with your credentials.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83d\udd11 SNOWFLAKE SESSION CONFIGURATION\n",
        "# Get Snowflake session\n",
        "# In Snowflake Notebooks, the session is automatically available\n",
        "try:\n",
        "    # For Snowflake Notebooks - session is provided automatically\n",
        "    session = context.get_active_session()\n",
        "    print(\"\u2705 Using Snowflake Notebooks session\")\n",
        "    session_type = \"snowflake_notebooks\"\n",
        "except:\n",
        "    # For local development - create session manually\n",
        "    print(\"\ud83d\udd27 Creating manual session for local development\")\n",
        "    session_type = \"local_development\"\n",
        "    \n",
        "    # Uncomment and update these parameters for local development:\n",
        "    # connection_parameters = {\n",
        "    #     \"account\": \"your_account\",\n",
        "    #     \"user\": \"your_username\",  \n",
        "    #     \"password\": \"your_password\",\n",
        "    #     \"role\": \"ACCOUNTADMIN\",\n",
        "    #     \"warehouse\": \"COMPUTE_WH\",\n",
        "    #     \"database\": \"CYBERSECURITY_DEMO\",\n",
        "    #     \"schema\": \"SECURITY_AI\"\n",
        "    # }\n",
        "    # session = Session.builder.configs(connection_parameters).create()\n",
        "    \n",
        "    print(\"\u274c No session available. Please configure connection_parameters above for local development.\")\n",
        "    session = None\n",
        "\n",
        "if session:\n",
        "    print(f\"\ud83d\udcca Session type: {session_type}\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f  No active session. Please configure connection for local development.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83d\udd0d SESSION VALIDATION & CONTEXT SETUP\n",
        "# Test Snowflake session and set context\n",
        "if session:\n",
        "    try:\n",
        "        # Set the correct database and schema context\n",
        "        session.sql(\"USE DATABASE CYBERSECURITY_DEMO\").collect()\n",
        "        session.sql(\"USE SCHEMA SECURITY_AI\").collect()\n",
        "        \n",
        "        # Test connection with a simple query\n",
        "        result = session.sql(\"SELECT CURRENT_DATABASE(), CURRENT_SCHEMA(), CURRENT_USER()\").collect()\n",
        "        print(f\"\u2705 Session active and connected!\")\n",
        "        print(f\"\ud83d\udd0d Current context: {result[0][0]}.{result[0][1]} as {result[0][2]}\")\n",
        "        \n",
        "        session_ready = True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Session test failed: {str(e)}\")\n",
        "        print(\"\ud83d\udd27 Please ensure the CYBERSECURITY_DEMO database and SECURITY_AI schema exist.\")\n",
        "        session_ready = False\n",
        "else:\n",
        "    print(\"\u274c No session available\")\n",
        "    session_ready = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udcda Model Registry Enterprise Setup\n",
        "\n",
        "## Snowflake Model Registry Configuration\n",
        "\n",
        "**\u2728 Enterprise Model Management**: Set up the Snowflake Model Registry for professional ML model lifecycle management.\n",
        "\n",
        "### Benefits:\n",
        "- **\ud83d\udcdd Version Control**: Track model versions and changes\n",
        "- **\ud83d\udcca Metadata Management**: Store training metrics and model information  \n",
        "- **\ud83d\udd12 Access Control**: Role-based permissions for model access\n",
        "- **\ud83d\udd04 Model Lineage**: Track model relationships and dependencies\n",
        "- **\ud83d\ude80 Auto-Deployment**: Seamless deployment as UDFs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83d\udcda MODEL REGISTRY INITIALIZATION\n",
        "# Initialize Snowflake Model Registry\n",
        "if session_ready:\n",
        "    try:\n",
        "        # Initialize the Model Registry\n",
        "        registry = Registry(\n",
        "            session=session,\n",
        "            database_name=\"CYBERSECURITY_DEMO\",\n",
        "            schema_name=\"SECURITY_AI\"\n",
        "        )\n",
        "        \n",
        "        print(\"\u2705 Model Registry initialized successfully!\")\n",
        "        print(f\"\ud83d\udccd Registry location: CYBERSECURITY_DEMO.SECURITY_AI\")\n",
        "        \n",
        "        # List existing models (if any)\n",
        "        try:\n",
        "            models = registry.show_models()\n",
        "            if len(models) > 0:\n",
        "                print(f\"\ud83d\udcda Found {len(models)} existing models in registry\")\n",
        "                for model in models:\n",
        "                    print(f\"  \ud83d\udcd6 {model}\")\n",
        "            else:\n",
        "                print(\"\ud83d\udcdd Registry is empty - ready for new models\")\n",
        "        except:\n",
        "            print(\"\ud83d\udcdd Registry initialized - ready for first models\")\n",
        "            \n",
        "        registry_ready = True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Model Registry initialization failed: {str(e)}\")\n",
        "        print(\"\ud83d\udca1 Ensure you have proper permissions and snowflake-ml-python is installed\")\n",
        "        registry_ready = False\n",
        "        registry = None\n",
        "else:\n",
        "    print(\"\u274c Skipping Model Registry setup - session not ready\")\n",
        "    registry_ready = False\n",
        "    registry = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udcca Pre-Training Data Validation\n",
        "\n",
        "## Data Quality and Readiness Check\n",
        "\n",
        "Before training ML models, let's validate that we have sufficient, high-quality data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83d\udcca DATA QUALITY VALIDATION\n",
        "# Check ML training data readiness\n",
        "def validate_training_data(session: Session) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Validate that sufficient data exists for ML training\n",
        "    \"\"\"\n",
        "    print(\"\ud83d\udd0d Validating training data readiness...\")\n",
        "    \n",
        "    try:\n",
        "        # Check overall data volume\n",
        "        validation_query = \"\"\"\n",
        "        SELECT \n",
        "            COUNT(*) as total_events,\n",
        "            COUNT(DISTINCT username) as unique_users,\n",
        "            COUNT(DISTINCT DATE(timestamp)) as training_days,\n",
        "            ROUND(COUNT(*) / COUNT(DISTINCT username), 2) as avg_events_per_user,\n",
        "            COUNT(DISTINCT location:country::STRING) as unique_countries,\n",
        "            COUNT(DISTINCT source_ip) as unique_ips,\n",
        "            ROUND(AVG(CASE WHEN success THEN 1.0 ELSE 0.0 END), 3) as success_rate\n",
        "        FROM USER_AUTHENTICATION_LOGS\n",
        "        WHERE timestamp >= DATEADD(day, -90, CURRENT_TIMESTAMP())\n",
        "        \"\"\"\n",
        "        \n",
        "        result = session.sql(validation_query).collect()[0]\n",
        "        \n",
        "        metrics = {\n",
        "            'total_events': result['TOTAL_EVENTS'],\n",
        "            'unique_users': result['UNIQUE_USERS'], \n",
        "            'training_days': result['TRAINING_DAYS'],\n",
        "            'avg_events_per_user': result['AVG_EVENTS_PER_USER'],\n",
        "            'unique_countries': result['UNIQUE_COUNTRIES'],\n",
        "            'unique_ips': result['UNIQUE_IPS'],\n",
        "            'success_rate': result['SUCCESS_RATE']\n",
        "        }\n",
        "        \n",
        "        return metrics\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Data validation failed: {str(e)}\")\n",
        "        return {}\n",
        "\n",
        "# Run validation\n",
        "if session_ready:\n",
        "    data_metrics = validate_training_data(session)\n",
        "else:\n",
        "    data_metrics = {}\n",
        "\n",
        "if data_metrics:\n",
        "    print(\"\\n\ud83d\udcca Training Data Summary:\")\n",
        "    print(f\"  \ud83d\udcc8 Total Events: {data_metrics['total_events']:,}\")\n",
        "    print(f\"  \ud83d\udc65 Unique Users: {data_metrics['unique_users']:,}\")\n",
        "    print(f\"  \ud83d\udcc5 Training Days: {data_metrics['training_days']}\")\n",
        "    print(f\"  \ud83d\udcca Avg Events/User: {data_metrics['avg_events_per_user']}\")\n",
        "    print(f\"  \ud83c\udf0d Countries: {data_metrics['unique_countries']}\")\n",
        "    print(f\"  \ud83c\udf10 Unique IPs: {data_metrics['unique_ips']:,}\")\n",
        "    print(f\"  \u2705 Success Rate: {data_metrics['success_rate']:.1%}\")\n",
        "    \n",
        "    # Determine readiness\n",
        "    if (data_metrics['total_events'] >= 100000 and \n",
        "        data_metrics['unique_users'] >= 100 and \n",
        "        data_metrics['training_days'] >= 60):\n",
        "        print(\"\\n\u2705 Data is READY for ML training!\")\n",
        "        training_ready = True\n",
        "    elif (data_metrics['total_events'] >= 10000 and \n",
        "          data_metrics['unique_users'] >= 50):\n",
        "        print(\"\\n\u26a0\ufe0f  Data is MINIMAL but usable for ML training.\")\n",
        "        training_ready = True\n",
        "    else:\n",
        "        print(\"\\n\u274c INSUFFICIENT data for ML training.\")\n",
        "        print(\"   Please ensure you've run the sample data generation script.\")\n",
        "        training_ready = False\n",
        "else:\n",
        "    training_ready = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\udd16 Enterprise ML Training Pipeline\n",
        "\n",
        "## Enhanced ML Training with Model Registry\n",
        "\n",
        "### \u26a0\ufe0f **Important: Model Registry Deployment Strategy**\n",
        "\n",
        "**After running this cell, your models will be:**\n",
        "- \u2705 **Registered** in Snowflake Model Registry with version control\n",
        "- \u2705 **Auto-deployed** as UDFs (e.g., `CYBERSECURITY_ISOLATION_FOREST_PREDICT`)\n",
        "- \u2705 **Ready for production** use in the Streamlit demo\n",
        "- \u2705 **Persistent** - no need to re-run unless retraining\n",
        "\n",
        "### \ud83d\udd04 **When to Re-run This Notebook:**\n",
        "- **New training data** available (monthly/quarterly retraining)\n",
        "- **Model performance** degradation detected\n",
        "- **Algorithm updates** or hyperparameter tuning needed\n",
        "- **New model versions** for A/B testing\n",
        "\n",
        "### \ud83c\udfaf **Training Pipeline:**\n",
        "1. Extract user behavior features from 90+ days of data\n",
        "2. Train Isolation Forest for anomaly detection  \n",
        "3. Train K-means for user clustering\n",
        "4. **Register models in Snowflake Model Registry** \ud83d\udcda\n",
        "5. **Deploy models as versioned UDFs** \ud83d\ude80\n",
        "6. Add metadata and performance tracking\n",
        "\n",
        "**Run this cell to train and deploy your real ML models!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83e\udd16 ENTERPRISE ML TRAINING & DEPLOYMENT\n",
        "if session_ready and training_ready and registry_ready:\n",
        "    print(\"\ud83d\ude80 Starting complete ML training and deployment pipeline...\")\n",
        "    \n",
        "    # 1. Extract user behavior features\n",
        "    print(\"\\n\ud83d\udcca Step 1: Feature Extraction\")\n",
        "    feature_query = \"\"\"\n",
        "    SELECT \n",
        "        username,\n",
        "        AVG(EXTRACT(HOUR FROM timestamp)) as avg_login_hour,\n",
        "        COALESCE(STDDEV(EXTRACT(HOUR FROM timestamp)), 0) as stddev_login_hour,\n",
        "        COUNT(*) as total_logins,\n",
        "        COUNT(DISTINCT source_ip) as unique_ips,\n",
        "        COUNT(DISTINCT location:country::STRING) as countries,\n",
        "        AVG(CASE WHEN EXTRACT(DOW FROM timestamp) IN (0,6) THEN 1.0 ELSE 0.0 END) as weekend_ratio,\n",
        "        AVG(CASE WHEN EXTRACT(HOUR FROM timestamp) BETWEEN 22 AND 6 THEN 1.0 ELSE 0.0 END) as offhours_ratio\n",
        "    FROM USER_AUTHENTICATION_LOGS\n",
        "    WHERE timestamp >= DATEADD(day, -90, CURRENT_TIMESTAMP())\n",
        "      AND username IS NOT NULL\n",
        "    GROUP BY username\n",
        "    HAVING COUNT(*) >= 10\n",
        "    \"\"\"\n",
        "    \n",
        "    training_df = session.sql(feature_query).to_pandas().fillna(0)\n",
        "    print(f\"\u2705 Extracted features for {len(training_df)} users\")\n",
        "    \n",
        "    # 2. Train Isolation Forest\n",
        "    print(\"\\n\ud83c\udf32 Step 2: Training Isolation Forest\")\n",
        "    feature_cols = ['avg_login_hour', 'stddev_login_hour', 'unique_ips', 'countries', 'weekend_ratio', 'offhours_ratio']\n",
        "    X = training_df[feature_cols]\n",
        "    \n",
        "    # Standardize features\n",
        "    isolation_scaler = StandardScaler()\n",
        "    X_scaled = isolation_scaler.fit_transform(X)\n",
        "    \n",
        "    # Train model\n",
        "    isolation_model = IsolationForest(contamination=0.1, random_state=42, n_estimators=100)\n",
        "    isolation_model.fit(X_scaled)\n",
        "    \n",
        "    # Get results\n",
        "    scores = isolation_model.decision_function(X_scaled)\n",
        "    anomalies = isolation_model.predict(X_scaled)\n",
        "    n_anomalies = sum(anomalies == -1)\n",
        "    \n",
        "    print(f\"\u2705 Isolation Forest trained: {n_anomalies} anomalies detected ({n_anomalies/len(training_df):.1%})\")\n",
        "    \n",
        "    # 3. Train K-means\n",
        "    print(\"\\n\ud83c\udfaf Step 3: Training K-means Clustering\") \n",
        "    cluster_features = ['avg_login_hour', 'countries', 'weekend_ratio', 'offhours_ratio', 'unique_ips']\n",
        "    X_cluster = training_df[cluster_features]\n",
        "    \n",
        "    # Standardize features\n",
        "    kmeans_scaler = StandardScaler()\n",
        "    X_cluster_scaled = kmeans_scaler.fit_transform(X_cluster)\n",
        "    \n",
        "    # Train model\n",
        "    kmeans_model = KMeans(n_clusters=6, random_state=42, n_init=10)\n",
        "    clusters = kmeans_model.fit_predict(X_cluster_scaled)\n",
        "    \n",
        "    print(f\"\u2705 K-means trained: {len(np.unique(clusters))} behavioral clusters created\")\n",
        "    \n",
        "    # 4. Register models in Snowflake Model Registry\n",
        "    print(\"\\n\ud83d\udcda Step 4: Registering Models in Model Registry\")\n",
        "    \n",
        "    # Create sample input data for model signatures\n",
        "    sample_input = X.iloc[:5]  # First 5 rows for model signature\n",
        "    \n",
        "    # Generate model version with timestamp\n",
        "    model_version = f\"v_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    \n",
        "    # Register Isolation Forest model\n",
        "    print(\"  \ud83c\udf32 Registering Isolation Forest model...\")\n",
        "    isolation_model_ref = registry.log_model(\n",
        "        model_name=\"cybersecurity_isolation_forest\",\n",
        "        version_name=model_version,\n",
        "        model=isolation_model,\n",
        "        sample_input_data=sample_input,\n",
        "        metadata={\n",
        "            \"model_type\": \"anomaly_detection\",\n",
        "            \"algorithm\": \"isolation_forest\",\n",
        "            \"contamination\": 0.1,\n",
        "            \"n_estimators\": 100,\n",
        "            \"training_samples\": len(training_df),\n",
        "            \"features\": feature_cols,\n",
        "            \"anomalies_detected\": n_anomalies,\n",
        "            \"anomaly_rate\": f\"{n_anomalies/len(training_df):.1%}\",\n",
        "            \"trained_at\": datetime.now().isoformat(),\n",
        "            \"purpose\": \"cybersecurity_user_behavior_anomaly_detection\"\n",
        "        }\n",
        "    )\n",
        "    print(f\"  \u2705 Isolation Forest registered as {model_version}\")\n",
        "    \n",
        "    # Register K-means model  \n",
        "    print(\"  \ud83c\udfaf Registering K-means clustering model...\")\n",
        "    cluster_sample = X_cluster.iloc[:5]  # Sample for clustering model\n",
        "    kmeans_model_ref = registry.log_model(\n",
        "        model_name=\"cybersecurity_kmeans_clustering\", \n",
        "        version_name=model_version,\n",
        "        model=kmeans_model,\n",
        "        sample_input_data=cluster_sample,\n",
        "        metadata={\n",
        "            \"model_type\": \"clustering\",\n",
        "            \"algorithm\": \"kmeans\",\n",
        "            \"n_clusters\": 6,\n",
        "            \"n_init\": 10,\n",
        "            \"training_samples\": len(training_df),\n",
        "            \"features\": cluster_features,\n",
        "            \"trained_at\": datetime.now().isoformat(),\n",
        "            \"purpose\": \"cybersecurity_user_behavior_clustering\"\n",
        "        }\n",
        "    )\n",
        "    print(f\"  \u2705 K-means registered as {model_version}\")\n",
        "    \n",
        "    # Register scalers as well (important for preprocessing)\n",
        "    print(\"  \ud83d\udccf Registering feature scalers...\")\n",
        "    scaler_sample = X.iloc[:1]  # Single row for scaler\n",
        "    isolation_scaler_ref = registry.log_model(\n",
        "        model_name=\"cybersecurity_isolation_scaler\",\n",
        "        version_name=model_version, \n",
        "        model=isolation_scaler,\n",
        "        sample_input_data=scaler_sample,\n",
        "        metadata={\n",
        "            \"model_type\": \"preprocessor\",\n",
        "            \"scaler_type\": \"StandardScaler\",\n",
        "            \"purpose\": \"isolation_forest_feature_scaling\"\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    kmeans_scaler_ref = registry.log_model(\n",
        "        model_name=\"cybersecurity_kmeans_scaler\",\n",
        "        version_name=model_version,\n",
        "        model=kmeans_scaler, \n",
        "        sample_input_data=cluster_sample,\n",
        "        metadata={\n",
        "            \"model_type\": \"preprocessor\", \n",
        "            \"scaler_type\": \"StandardScaler\",\n",
        "            \"purpose\": \"kmeans_feature_scaling\"\n",
        "        }\n",
        "    )\n",
        "    print(f\"  \u2705 Feature scalers registered\")\n",
        "    \n",
        "    # 5. Deploy models as UDFs (automatic with Model Registry)\n",
        "    print(\"\\n\ud83d\ude80 Step 5: Deploying Models as UDFs\")\n",
        "    try:\n",
        "        # Deploy Isolation Forest for inference\n",
        "        print(\"  \ud83c\udf32 Deploying Isolation Forest UDF...\")\n",
        "        isolation_model_ref.create_udf(\n",
        "            udf_name=\"cybersecurity_isolation_forest_predict\",\n",
        "            replace_if_exists=True\n",
        "        )\n",
        "        \n",
        "        # Deploy K-means for inference  \n",
        "        print(\"  \ud83c\udfaf Deploying K-means UDF...\")\n",
        "        kmeans_model_ref.create_udf(\n",
        "            udf_name=\"cybersecurity_kmeans_predict\", \n",
        "            replace_if_exists=True\n",
        "        )\n",
        "        \n",
        "        print(\"  \u2705 Models deployed as UDFs successfully!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  \u26a0\ufe0f  UDF deployment: {str(e)}\")\n",
        "        print(\"  \ud83d\udca1 UDFs can be created manually from registered models\")\n",
        "        \n",
        "    # 6. Model Registry Validation\n",
        "    print(\"\\n\ud83d\udd0d Step 6: Model Registry Validation\")\n",
        "    try:\n",
        "        # List all models in registry\n",
        "        models = registry.show_models()\n",
        "        print(f\"\u2705 {len(models)} models registered in Model Registry\")\n",
        "        \n",
        "        # Show model details\n",
        "        for model_name in [\"cybersecurity_isolation_forest\", \"cybersecurity_kmeans_clustering\"]:\n",
        "            try:\n",
        "                model_info = registry.get_model(model_name)\n",
        "                print(f\"  \ud83d\udcd6 {model_name}: {model_info}\")\n",
        "            except:\n",
        "                print(f\"  \u26a0\ufe0f  Model {model_name} not found\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Registry validation error: {str(e)}\")\n",
        "    \n",
        "    # Final summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"\ud83c\udf89 ENTERPRISE ML IMPLEMENTATION WITH MODEL REGISTRY COMPLETE!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\ud83d\udcca Training Data: {len(training_df):,} users\")\n",
        "    print(f\"\ud83c\udf32 Isolation Forest: {n_anomalies} anomalies ({n_anomalies/len(training_df):.1%})\")\n",
        "    print(f\"\ud83c\udfaf K-means: {len(np.unique(clusters))} behavioral clusters\") \n",
        "    print(f\"\ud83d\udcda Model Registry: \u2705 4 models registered with metadata\")\n",
        "    print(f\"\ud83d\ude80 UDF Deployment: \u2705 Models available as SQL functions\")\n",
        "    print(f\"\ud83d\udcdd Model Version: {model_version}\")\n",
        "    print(\"\\n\u2728 Model Registry Benefits:\")\n",
        "    print(\"  \ud83d\udcdd Version control and lineage tracking\")\n",
        "    print(\"  \ud83d\udcca Rich metadata and performance metrics\")\n",
        "    print(\"  \ud83d\udd12 Role-based access control\") \n",
        "    print(\"  \ud83d\udd04 Automated deployment pipeline\")\n",
        "    print(\"\\n\ud83c\udfaf Next Steps:\")\n",
        "    print(\"1. Test models: SELECT cybersecurity_isolation_forest_predict(...)\")\n",
        "    print(\"2. Update UDFs in SQL scripts to use Registry models\")\n",
        "    print(\"3. Launch: Your Streamlit app now uses Enterprise ML!\")\n",
        "    print(\"\\n\ud83d\ude80 This is production-grade, enterprise ML with full lifecycle management!\")\n",
        "    \n",
        "    # Post-deployment guidance\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"\ud83d\udccb NEXT STEPS AFTER MODEL DEPLOYMENT\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"1. \ud83c\udfaf Your Streamlit demo now uses REAL ML models automatically\")\n",
        "    print(\"2. \ud83d\udcca Models are persistent - no need to re-run this notebook regularly\")\n",
        "    print(\"3. \ud83d\udd0d Use 'SHOW MODELS IN MODEL REGISTRY' to view your models\")\n",
        "    print(\"4. \ud83d\udcc8 Monitor model performance in production\")\n",
        "    print(\"5. \ud83d\udd04 Re-run this notebook only for model updates/retraining\")\n",
        "    print(\"\\n\ud83d\udca1 TIP: You can now focus on using the Streamlit demo!\")\n",
        "    print(\"   The heavy ML work is done and deployed.\")\n",
        "    \n",
        "else:\n",
        "    if not session_ready:\n",
        "        print(\"\u274c Skipping ML training due to session issues.\")\n",
        "        print(\"\ud83d\udca1 Please ensure Snowflake session is properly configured.\")\n",
        "    elif not training_ready:\n",
        "        print(\"\u274c Skipping ML training due to insufficient data.\")\n",
        "        print(\"\ud83d\udca1 Please run the SQL data generation scripts first.\")\n",
        "    elif not registry_ready:\n",
        "        print(\"\u274c Skipping ML training due to Model Registry issues.\")\n",
        "        print(\"\ud83d\udca1 Please ensure snowflake-ml-python is installed and permissions are correct.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udcda Model Registry Best Practices\n",
        "\n",
        "## Enterprise ML Lifecycle Management\n",
        "\n",
        "### \u2728 **Your Models Are Now Enterprise-Grade**\n",
        "\n",
        "Your models are now managed with enterprise-grade practices:\n",
        "\n",
        "#### **\ud83d\udd12 Governance & Security**\n",
        "- **Role-based Access**: Control who can view/modify models\n",
        "- **Audit Trails**: Track all model changes and deployments\n",
        "- **Version Control**: Rollback to previous model versions\n",
        "- **Metadata Management**: Rich model documentation and lineage\n",
        "\n",
        "#### **\ud83d\ude80 Operational Excellence**\n",
        "- **Auto-Deployment**: Models become UDFs automatically\n",
        "- **Performance Tracking**: Monitor model accuracy over time\n",
        "- **A/B Testing**: Deploy multiple model versions simultaneously  \n",
        "- **CI/CD Integration**: Automated model deployment pipelines\n",
        "\n",
        "#### **\ud83d\udc65 Team Collaboration**\n",
        "- **Model Sharing**: Team access to registered models\n",
        "- **Documentation**: Built-in model descriptions and metrics\n",
        "- **Change Management**: Track who trained/deployed which models\n",
        "- **Knowledge Transfer**: Onboard new team members easily\n",
        "\n",
        "### \ud83c\udfaf **Recommended Workflow**\n",
        "\n",
        "1. **Initial Setup**: Run this notebook once to train and register models\n",
        "2. **Production Use**: Streamlit demo automatically uses registered models\n",
        "3. **Monitoring**: Track model performance in production dashboards\n",
        "4. **Retraining**: Re-run notebook monthly/quarterly for fresh models\n",
        "5. **Version Management**: Use Model Registry to manage model lifecycle\n",
        "\n",
        "### \ud83d\udca1 **Pro Tips**\n",
        "- Models persist across Snowflake sessions - no need for frequent retraining\n",
        "- Use versioning for gradual model rollouts and A/B testing\n",
        "- Monitor data drift and retrain models when performance degrades\n",
        "- Leverage Model Registry metadata for model documentation and compliance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u2705 **Enterprise ML Training Complete!**\n",
        "\n",
        "### \ud83c\udf89 **Congratulations!** Your Snowflake Cybersecurity AI Platform is now **production-ready** with:\n",
        "\n",
        "- **\u2705 Complete Database Schema** - Users, logs, vulnerabilities, threat intelligence\n",
        "- **\u2705 Realistic Sample Data** - 500+ users, 180+ days of telemetry  \n",
        "- **\u2705 Production ML Models** - Isolation Forest, K-means clustering\n",
        "- **\u2705 Model Registry Integration** - Enterprise lifecycle management\n",
        "- **\u2705 Advanced ML UDFs** - Real-time anomaly detection\n",
        "- **\u2705 Cortex AI Integration** - Natural language threat analysis\n",
        "\n",
        "### \ud83d\ude80 **Next Steps:**\n",
        "1. **Deploy Streamlit App** - Use the companion Python application\n",
        "2. **Explore Dashboards** - Analyze security metrics and ML insights\n",
        "3. **Test Anomaly Detection** - Inject test data to validate models\n",
        "4. **Scale Training** - Add more historical data for improved accuracy\n",
        "\n",
        "---\n",
        "\n",
        "**\ud83d\udee1\ufe0f Your cybersecurity AI platform is ready to defend against advanced threats!**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}