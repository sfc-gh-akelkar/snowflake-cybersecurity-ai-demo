{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ›¡ï¸ Cybersecurity ML Training & Deployment\n",
        "\n",
        "This notebook demonstrates advanced machine learning for cybersecurity using **Snowpark ML** and **Snowflake Model Registry**.\n",
        "\n",
        "## ðŸŽ¯ What This Notebook Does\n",
        "\n",
        "1. **Isolation Forest**: Detects anomalous user behavior patterns\n",
        "2. **K-means Clustering**: Groups users into behavioral personas  \n",
        "3. **Model Registry**: Enterprise-grade model management\n",
        "4. **UDF Deployment**: Deploy models as scalable functions\n",
        "5. **Hybrid Analysis**: Combine multiple ML approaches\n",
        "\n",
        "## ðŸ“‹ Prerequisites\n",
        "\n",
        "Before running this notebook, ensure you have:\n",
        "1. âœ… Run `sql/01_cybersecurity_schema.sql` \n",
        "2. âœ… Run `sql/02_sample_data_generation.sql`\n",
        "3. âœ… Run `sql/03_native_ml_and_cortex.sql`\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”§ Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import snowflake.snowpark as snowpark\n",
        "from snowflake.snowpark import Session\n",
        "from snowflake.snowpark.functions import col, lit, when, avg, count, max as max_, min as min_\n",
        "from snowflake.snowpark.types import StructType, StructField, StringType, FloatType, IntegerType, BooleanType\n",
        "\n",
        "# Snowpark ML imports\n",
        "from snowflake.ml.modeling.ensemble import IsolationForest\n",
        "from snowflake.ml.modeling.cluster import KMeans\n",
        "from snowflake.ml.modeling.preprocessing import StandardScaler\n",
        "from snowflake.ml.registry import Registry\n",
        "\n",
        "# Data science libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "\n",
        "print(\"ðŸ“¦ All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Snowflake session (auto-connects in Snowflake Notebooks)\n",
        "session = snowpark.context.get_active_session()\n",
        "\n",
        "# Set up database context\n",
        "session.sql(\"USE DATABASE CYBERSECURITY_DEMO\").collect()\n",
        "session.sql(\"USE SCHEMA SECURITY_ANALYTICS\").collect()\n",
        "session.sql(\"USE WAREHOUSE CYBERSECURITY_WH\").collect()\n",
        "\n",
        "print(\"ðŸ”— Connected to Snowflake session\")\n",
        "print(f\"ðŸ“Š Current database: {session.get_current_database()}\")\n",
        "print(f\"ðŸ“ Current schema: {session.get_current_schema()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š Data Preparation for ML\n",
        "\n",
        "*The ML models require feature engineering to extract behavioral patterns from raw authentication logs.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create feature engineering view for ML models with explicit type casting\n",
        "feature_query = \"\"\"\n",
        "CREATE OR REPLACE VIEW ML_FEATURE_SET AS\n",
        "SELECT \n",
        "    ual.USERNAME,\n",
        "    \n",
        "    -- Behavioral features (cast to avoid decimal precision warnings)\n",
        "    COUNT(*)::INTEGER as TOTAL_LOGINS,\n",
        "    AVG(CASE WHEN ual.SUCCESS THEN 1 ELSE 0 END)::FLOAT as SUCCESS_RATE,\n",
        "    COUNT(CASE WHEN ual.SUCCESS = FALSE THEN 1 END)::INTEGER as FAILED_ATTEMPTS,\n",
        "    \n",
        "    -- Temporal features\n",
        "    COUNT(CASE WHEN EXTRACT(hour FROM ual.TIMESTAMP) BETWEEN 22 AND 6 THEN 1 END)::INTEGER as OFF_HOURS_LOGINS,\n",
        "    COUNT(CASE WHEN EXTRACT(dow FROM ual.TIMESTAMP) IN (0,6) THEN 1 END)::INTEGER as WEEKEND_LOGINS,\n",
        "    \n",
        "    -- Geographic features\n",
        "    COUNT(DISTINCT ual.SOURCE_IP)::INTEGER as UNIQUE_IPS,\n",
        "    COUNT(DISTINCT ual.LOCATION:country::STRING)::INTEGER as UNIQUE_COUNTRIES,\n",
        "    \n",
        "    -- Security features\n",
        "    AVG(CASE WHEN ual.TWO_FACTOR_USED THEN 1 ELSE 0 END)::FLOAT as TWO_FACTOR_RATE,\n",
        "    COUNT(DISTINCT ual.USER_AGENT)::INTEGER as UNIQUE_DEVICES,\n",
        "    \n",
        "    -- Organizational context\n",
        "    ed.DEPARTMENT,\n",
        "    ed.ROLE,\n",
        "    ed.SECURITY_CLEARANCE,\n",
        "    DATEDIFF(day, ed.HIRE_DATE, CURRENT_DATE())::INTEGER as TENURE_DAYS\n",
        "    \n",
        "FROM USER_AUTHENTICATION_LOGS ual\n",
        "JOIN EMPLOYEE_DATA ed ON ual.USERNAME = ed.USERNAME\n",
        "WHERE ual.TIMESTAMP >= DATEADD(day, -30, CURRENT_TIMESTAMP())\n",
        "    AND ed.STATUS = 'active'\n",
        "GROUP BY ual.USERNAME, ed.DEPARTMENT, ed.ROLE, ed.SECURITY_CLEARANCE, ed.HIRE_DATE\n",
        "HAVING COUNT(*) >= 5  -- Filter users with sufficient activity\n",
        "\"\"\"\n",
        "\n",
        "session.sql(feature_query).collect()\n",
        "print(\"ðŸ”§ Feature engineering view created with explicit type casting\")\n",
        "\n",
        "# Load feature data into Snowpark DataFrame\n",
        "feature_df = session.table('ML_FEATURE_SET')\n",
        "print(f\"ðŸ“ˆ Feature dataset: {feature_df.count()} users with behavioral features\")\n",
        "\n",
        "# Additional type casting in Snowpark DataFrame to ensure clean data types\n",
        "from snowflake.snowpark.types import IntegerType, FloatType\n",
        "\n",
        "# Cast numerical columns to appropriate types to prevent decimal conversion warnings\n",
        "feature_df = feature_df.with_column(\"TOTAL_LOGINS\", col(\"TOTAL_LOGINS\").cast(IntegerType())) \\\n",
        "                       .with_column(\"SUCCESS_RATE\", col(\"SUCCESS_RATE\").cast(FloatType())) \\\n",
        "                       .with_column(\"FAILED_ATTEMPTS\", col(\"FAILED_ATTEMPTS\").cast(IntegerType())) \\\n",
        "                       .with_column(\"OFF_HOURS_LOGINS\", col(\"OFF_HOURS_LOGINS\").cast(IntegerType())) \\\n",
        "                       .with_column(\"WEEKEND_LOGINS\", col(\"WEEKEND_LOGINS\").cast(IntegerType())) \\\n",
        "                       .with_column(\"UNIQUE_IPS\", col(\"UNIQUE_IPS\").cast(IntegerType())) \\\n",
        "                       .with_column(\"UNIQUE_COUNTRIES\", col(\"UNIQUE_COUNTRIES\").cast(IntegerType())) \\\n",
        "                       .with_column(\"TWO_FACTOR_RATE\", col(\"TWO_FACTOR_RATE\").cast(FloatType())) \\\n",
        "                       .with_column(\"UNIQUE_DEVICES\", col(\"UNIQUE_DEVICES\").cast(IntegerType())) \\\n",
        "                       .with_column(\"TENURE_DAYS\", col(\"TENURE_DAYS\").cast(IntegerType()))\n",
        "\n",
        "print(\"âœ… Data types explicitly cast to prevent precision warnings\")\n",
        "\n",
        "# Inspect and display data types to ensure all are correct\n",
        "print(\"\\nðŸ” Final data types inspection:\")\n",
        "feature_df.dtypes\n",
        "\n",
        "# Show sample of features with clean data types\n",
        "print(\"\\nðŸ“Š Sample of clean feature data:\")\n",
        "feature_df.limit(5).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ¤– Isolation Forest - Anomaly Detection\n",
        "\n",
        "*Uses unsupervised learning to identify users with anomalous behavior patterns that could indicate security threats.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to ensure clean data types and prevent decimal warnings\n",
        "def ensure_clean_datatypes(df, numerical_columns):\n",
        "    \"\"\"Ensures all numerical columns have appropriate data types for ML\"\"\"\n",
        "    from snowflake.snowpark.types import IntegerType, FloatType, DecimalType\n",
        "    \n",
        "    for col_name in numerical_columns:\n",
        "        current_type = dict(df.dtypes)[col_name]\n",
        "        print(f\"  {col_name}: {current_type}\")\n",
        "        \n",
        "        # Convert any remaining decimal types to appropriate types\n",
        "        if isinstance(current_type, DecimalType):\n",
        "            if col_name in ['SUCCESS_RATE', 'TWO_FACTOR_RATE']:\n",
        "                df = df.with_column(col_name, col(col_name).cast(FloatType()))\n",
        "                print(f\"    â†’ Converted {col_name} to FloatType\")\n",
        "            else:\n",
        "                df = df.with_column(col_name, col(col_name).cast(IntegerType()))\n",
        "                print(f\"    â†’ Converted {col_name} to IntegerType\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Prepare numerical features for Isolation Forest\n",
        "iso_features = [\n",
        "    'TOTAL_LOGINS', 'SUCCESS_RATE', 'FAILED_ATTEMPTS', \n",
        "    'OFF_HOURS_LOGINS', 'WEEKEND_LOGINS', 'UNIQUE_IPS', \n",
        "    'UNIQUE_COUNTRIES', 'TWO_FACTOR_RATE', 'UNIQUE_DEVICES', 'TENURE_DAYS'\n",
        "]\n",
        "\n",
        "print(\"ðŸ”§ Ensuring clean data types for ML features:\")\n",
        "feature_df = ensure_clean_datatypes(feature_df, iso_features)\n",
        "\n",
        "# Create Isolation Forest model\n",
        "isolation_forest = IsolationForest(\n",
        "    n_estimators=100,\n",
        "    contamination=0.1,  # Expect ~10% anomalies\n",
        "    random_state=42,\n",
        "    input_cols=iso_features,\n",
        "    output_cols=['ISOLATION_ANOMALY_SCORE', 'ISOLATION_IS_ANOMALY']\n",
        ")\n",
        "\n",
        "print(\"ðŸŒ² Training Isolation Forest model...\")\n",
        "\n",
        "# Train the model\n",
        "isolation_model = isolation_forest.fit(feature_df)\n",
        "\n",
        "print(\"âœ… Isolation Forest training completed!\")\n",
        "\n",
        "# Apply predictions to the dataset\n",
        "feature_df_with_iso = isolation_model.predict(feature_df)\n",
        "\n",
        "print(\"ðŸ” Anomaly detection results:\")\n",
        "anomaly_summary = feature_df_with_iso.group_by('ISOLATION_IS_ANOMALY').agg(count(lit(1)).alias('COUNT'))\n",
        "anomaly_summary.show()\n",
        "\n",
        "# Show sample anomalies\n",
        "print(\"\\nðŸš¨ Sample anomalous users:\")\n",
        "anomalies = feature_df_with_iso.filter(col('ISOLATION_IS_ANOMALY') == True).select(\n",
        "    'USERNAME', 'DEPARTMENT', 'TOTAL_LOGINS', 'SUCCESS_RATE', \n",
        "    'UNIQUE_COUNTRIES', 'ISOLATION_ANOMALY_SCORE'\n",
        ").limit(10)\n",
        "anomalies.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ‘¥ K-means Clustering - User Personas\n",
        "\n",
        "*Groups users into behavioral clusters to understand normal patterns and identify deviations from expected behavior.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features for clustering (normalize for better clustering)\n",
        "cluster_features = [\n",
        "    'TOTAL_LOGINS', 'SUCCESS_RATE', 'OFF_HOURS_LOGINS', \n",
        "    'WEEKEND_LOGINS', 'UNIQUE_IPS', 'UNIQUE_COUNTRIES', 'TWO_FACTOR_RATE'\n",
        "]\n",
        "\n",
        "print(\"ðŸ”§ Ensuring clean data types for clustering features:\")\n",
        "feature_df_with_iso = ensure_clean_datatypes(feature_df_with_iso, cluster_features)\n",
        "\n",
        "# Scale features for clustering\n",
        "scaler = StandardScaler(\n",
        "    input_cols=cluster_features,\n",
        "    output_cols=[f\"{col}_SCALED\" for col in cluster_features]\n",
        ")\n",
        "\n",
        "scaled_df = scaler.fit(feature_df_with_iso).transform(feature_df_with_iso)\n",
        "\n",
        "# Create K-means clustering model\n",
        "kmeans = KMeans(\n",
        "    n_clusters=4,  # 4 user personas: Normal, Power, Cautious, Suspicious\n",
        "    random_state=42,\n",
        "    input_cols=[f\"{col}_SCALED\" for col in cluster_features],\n",
        "    output_cols=['CLUSTER_LABEL', 'CLUSTER_DISTANCE']\n",
        ")\n",
        "\n",
        "print(\"ðŸ”„ Training K-means clustering model...\")\n",
        "\n",
        "# Train the clustering model\n",
        "kmeans_model = kmeans.fit(scaled_df)\n",
        "\n",
        "print(\"âœ… K-means clustering completed!\")\n",
        "\n",
        "# Apply clustering\n",
        "final_df = kmeans_model.predict(scaled_df)\n",
        "\n",
        "print(\"ðŸ‘¥ User cluster distribution:\")\n",
        "cluster_summary = final_df.group_by('CLUSTER_LABEL').agg(\n",
        "    count(lit(1)).alias('USER_COUNT'),\n",
        "    avg('TOTAL_LOGINS').alias('AVG_LOGINS'),\n",
        "    avg('SUCCESS_RATE').alias('AVG_SUCCESS_RATE'),\n",
        "    avg('UNIQUE_COUNTRIES').alias('AVG_COUNTRIES')\n",
        ")\n",
        "cluster_summary.show()\n",
        "\n",
        "# Analyze cluster characteristics\n",
        "print(\"\\nðŸ“Š Cluster characteristics by department:\")\n",
        "dept_clusters = final_df.group_by('DEPARTMENT', 'CLUSTER_LABEL').agg(\n",
        "    count(lit(1)).alias('COUNT')\n",
        ").pivot('CLUSTER_LABEL', ['0', '1', '2', '3']).agg({\n",
        "    '0': 'sum', '1': 'sum', '2': 'sum', '3': 'sum'\n",
        "}).fillna(0)\n",
        "dept_clusters.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š Model Registry - Enterprise ML Management\n",
        "\n",
        "*Deploy trained models to Snowflake Model Registry for enterprise-grade model lifecycle management.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Model Registry\n",
        "registry = Registry(session=session, database_name=\"CYBERSECURITY_DEMO\", schema_name=\"SECURITY_ANALYTICS\")\n",
        "\n",
        "print(\"ðŸ“‹ Registering models in Snowflake Model Registry...\")\n",
        "\n",
        "# Register Isolation Forest model\n",
        "iso_model_ref = registry.log_model(\n",
        "    model=isolation_model,\n",
        "    model_name=\"cybersecurity_isolation_forest\",\n",
        "    version_name=\"v1.0\",\n",
        "    comment=\"Isolation Forest for detecting anomalous user behavior patterns\",\n",
        "    tags={\"use_case\": \"anomaly_detection\", \"model_type\": \"isolation_forest\", \"department\": \"security\"}\n",
        ")\n",
        "\n",
        "print(f\"âœ… Isolation Forest registered: {iso_model_ref.fully_qualified_model_name}\")\n",
        "\n",
        "# Register K-means model  \n",
        "kmeans_model_ref = registry.log_model(\n",
        "    model=kmeans_model,\n",
        "    model_name=\"cybersecurity_user_clustering\", \n",
        "    version_name=\"v1.0\",\n",
        "    comment=\"K-means clustering for user behavioral personas\",\n",
        "    tags={\"use_case\": \"user_clustering\", \"model_type\": \"kmeans\", \"department\": \"security\"}\n",
        ")\n",
        "\n",
        "print(f\"âœ… K-means model registered: {kmeans_model_ref.fully_qualified_model_name}\")\n",
        "\n",
        "# Register feature scaler\n",
        "scaler_ref = registry.log_model(\n",
        "    model=scaler,\n",
        "    model_name=\"cybersecurity_feature_scaler\",\n",
        "    version_name=\"v1.0\", \n",
        "    comment=\"StandardScaler for normalizing cybersecurity features\",\n",
        "    tags={\"use_case\": \"preprocessing\", \"model_type\": \"scaler\", \"department\": \"security\"}\n",
        ")\n",
        "\n",
        "print(f\"âœ… Feature scaler registered: {scaler_ref.fully_qualified_model_name}\")\n",
        "\n",
        "# List all registered models\n",
        "print(\"\\nðŸ“š All models in registry:\")\n",
        "models = registry.list_models()\n",
        "for model in models:\n",
        "    print(f\"  ðŸ¤– {model}\")\n",
        "\n",
        "print(\"\\nðŸŽ¯ Model Registry deployment completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš¡ UDF Deployment - Production ML Functions\n",
        "\n",
        "*Deploy models as SQL UDFs for real-time scoring in production workloads.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deploy Isolation Forest as UDF for real-time anomaly detection\n",
        "iso_udf = iso_model_ref.run(\n",
        "    X=session.table('ML_FEATURE_SET').select(iso_features),\n",
        "    function_name=\"DETECT_USER_ANOMALIES\"\n",
        ")\n",
        "\n",
        "print(\"ðŸš¨ Isolation Forest UDF deployed: DETECT_USER_ANOMALIES\")\n",
        "\n",
        "# Deploy K-means clustering as UDF for user segmentation  \n",
        "cluster_udf = kmeans_model_ref.run(\n",
        "    X=scaler_ref.run(session.table('ML_FEATURE_SET').select(cluster_features)),\n",
        "    function_name=\"CLASSIFY_USER_BEHAVIOR\"\n",
        ")\n",
        "\n",
        "print(\"ðŸ‘¥ K-means clustering UDF deployed: CLASSIFY_USER_BEHAVIOR\")\n",
        "\n",
        "# Test the deployed UDFs\n",
        "print(\"\\nðŸ§ª Testing deployed UDFs...\")\n",
        "\n",
        "# Test anomaly detection UDF\n",
        "anomaly_test = session.sql(\"\"\"\n",
        "    SELECT \n",
        "        USERNAME,\n",
        "        DEPARTMENT,\n",
        "        TOTAL_LOGINS,\n",
        "        DETECT_USER_ANOMALIES(\n",
        "            TOTAL_LOGINS, SUCCESS_RATE, FAILED_ATTEMPTS, OFF_HOURS_LOGINS,\n",
        "            WEEKEND_LOGINS, UNIQUE_IPS, UNIQUE_COUNTRIES, TWO_FACTOR_RATE,\n",
        "            UNIQUE_DEVICES, TENURE_DAYS\n",
        "        ) as ANOMALY_PREDICTION\n",
        "    FROM ML_FEATURE_SET\n",
        "    LIMIT 5\n",
        "\"\"\")\n",
        "\n",
        "print(\"ðŸ” Anomaly Detection UDF test:\")\n",
        "anomaly_test.show()\n",
        "\n",
        "# Test clustering UDF  \n",
        "cluster_test = session.sql(\"\"\"\n",
        "    SELECT \n",
        "        USERNAME,\n",
        "        DEPARTMENT, \n",
        "        CLASSIFY_USER_BEHAVIOR(\n",
        "            TOTAL_LOGINS, SUCCESS_RATE, OFF_HOURS_LOGINS,\n",
        "            WEEKEND_LOGINS, UNIQUE_IPS, UNIQUE_COUNTRIES, TWO_FACTOR_RATE\n",
        "        ) as CLUSTER_PREDICTION\n",
        "    FROM ML_FEATURE_SET\n",
        "    LIMIT 5\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nðŸ‘¥ User Clustering UDF test:\")\n",
        "cluster_test.show()\n",
        "\n",
        "print(\"\\nâœ… UDF deployment and testing completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”„ Update Snowpark ML Results Tables\n",
        "\n",
        "*Populate the ML results tables that the Streamlit app uses for hybrid analysis.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Snowpark ML anomaly results table\n",
        "print(\"ðŸ“Š Creating SNOWPARK_ML_ANOMALY_RESULTS table...\")\n",
        "\n",
        "snowpark_anomaly_query = \"\"\"\n",
        "CREATE OR REPLACE TABLE SNOWPARK_ML_ANOMALY_RESULTS AS\n",
        "SELECT \n",
        "    USERNAME,\n",
        "    ISOLATION_ANOMALY_SCORE,\n",
        "    ISOLATION_IS_ANOMALY,\n",
        "    CURRENT_TIMESTAMP() as ANALYSIS_TIMESTAMP\n",
        "FROM ML_FEATURE_SET f\n",
        "JOIN ({}) ml ON f.USERNAME = ml.USERNAME\n",
        "\"\"\".format(\n",
        "    final_df.select('USERNAME', 'ISOLATION_ANOMALY_SCORE', 'ISOLATION_IS_ANOMALY').queries['queries'][0]\n",
        ")\n",
        "\n",
        "session.sql(snowpark_anomaly_query).collect()\n",
        "\n",
        "# Create Snowpark ML clustering results table\n",
        "print(\"ðŸ‘¥ Creating SNOWPARK_ML_USER_CLUSTERS table...\")\n",
        "\n",
        "snowpark_cluster_query = \"\"\"\n",
        "CREATE OR REPLACE TABLE SNOWPARK_ML_USER_CLUSTERS AS\n",
        "SELECT \n",
        "    f.USERNAME,\n",
        "    f.DEPARTMENT,\n",
        "    f.TOTAL_LOGINS,\n",
        "    f.SUCCESS_RATE,\n",
        "    f.UNIQUE_COUNTRIES,\n",
        "    f.WEEKEND_LOGINS,\n",
        "    ml.CLUSTER_LABEL,\n",
        "    ml.CLUSTER_DISTANCE,\n",
        "    CURRENT_TIMESTAMP() as ANALYSIS_TIMESTAMP\n",
        "FROM ML_FEATURE_SET f\n",
        "JOIN ({}) ml ON f.USERNAME = ml.USERNAME\n",
        "\"\"\".format(\n",
        "    final_df.select('USERNAME', 'CLUSTER_LABEL', 'CLUSTER_DISTANCE').queries['queries'][0]\n",
        ")\n",
        "\n",
        "session.sql(snowpark_cluster_query).collect()\n",
        "\n",
        "# Update the ML_MODEL_COMPARISON view with real Snowpark ML results\n",
        "print(\"ðŸ”„ Updating ML_MODEL_COMPARISON view...\")\n",
        "\n",
        "update_comparison_view = \"\"\"\n",
        "CREATE OR REPLACE VIEW ML_MODEL_COMPARISON AS\n",
        "SELECT\n",
        "    n.USERNAME,\n",
        "    ed.DEPARTMENT,\n",
        "    ed.ROLE,\n",
        "    CURRENT_TIMESTAMP() as ANALYSIS_DATE,\n",
        "\n",
        "    -- Native ML Results \n",
        "    COALESCE(n.IS_ANOMALY, FALSE) as NATIVE_IS_ANOMALY,\n",
        "    COALESCE(n.FORECAST, 0) as NATIVE_ANOMALY_SCORE,\n",
        "\n",
        "    -- Snowpark ML Results (now with real data!)\n",
        "    COALESCE(s.ISOLATION_IS_ANOMALY, FALSE) as ISOLATION_FOREST_ANOMALY,\n",
        "    COALESCE(s.ISOLATION_ANOMALY_SCORE, 0.0) as ISOLATION_FOREST_SCORE,\n",
        "    COALESCE(c.CLUSTER_LABEL, 0) as CLUSTER_LABEL,\n",
        "    COALESCE(c.CLUSTER_DISTANCE, 0.0) as CLUSTER_DISTANCE,\n",
        "\n",
        "    -- Enhanced Risk Assessment using both models\n",
        "    CASE\n",
        "        WHEN COALESCE(n.IS_ANOMALY, FALSE) = TRUE AND COALESCE(s.ISOLATION_IS_ANOMALY, FALSE) = TRUE THEN 'CRITICAL'\n",
        "        WHEN COALESCE(n.IS_ANOMALY, FALSE) = TRUE OR COALESCE(s.ISOLATION_IS_ANOMALY, FALSE) = TRUE THEN 'HIGH'\n",
        "        WHEN n.FORECAST IS NOT NULL OR s.ISOLATION_ANOMALY_SCORE > 0.5 THEN 'MEDIUM'\n",
        "        ELSE 'LOW'\n",
        "    END as RISK_LEVEL,\n",
        "\n",
        "    -- Model agreement analysis\n",
        "    CASE\n",
        "        WHEN COALESCE(n.IS_ANOMALY, FALSE) = TRUE AND COALESCE(s.ISOLATION_IS_ANOMALY, FALSE) = TRUE THEN 'BOTH_AGREE_ANOMALY'\n",
        "        WHEN COALESCE(n.IS_ANOMALY, FALSE) = FALSE AND COALESCE(s.ISOLATION_IS_ANOMALY, FALSE) = FALSE THEN 'BOTH_AGREE_NORMAL'\n",
        "        WHEN COALESCE(n.IS_ANOMALY, FALSE) = TRUE THEN 'NATIVE_ONLY'\n",
        "        WHEN COALESCE(s.ISOLATION_IS_ANOMALY, FALSE) = TRUE THEN 'SNOWPARK_ONLY'\n",
        "        ELSE 'INSUFFICIENT_DATA'\n",
        "    END as MODEL_AGREEMENT\n",
        "\n",
        "FROM NATIVE_ML_ANOMALY_RESULTS n\n",
        "FULL OUTER JOIN SNOWPARK_ML_ANOMALY_RESULTS s ON n.USERNAME = s.USERNAME  \n",
        "FULL OUTER JOIN SNOWPARK_ML_USER_CLUSTERS c ON COALESCE(n.USERNAME, s.USERNAME) = c.USERNAME\n",
        "JOIN EMPLOYEE_DATA ed ON COALESCE(n.USERNAME, s.USERNAME, c.USERNAME) = ed.USERNAME\n",
        "WHERE ed.STATUS = 'active'\n",
        "\"\"\"\n",
        "\n",
        "session.sql(update_comparison_view).collect()\n",
        "\n",
        "print(\"âœ… Snowpark ML tables created and ML_MODEL_COMPARISON view updated!\")\n",
        "print(\"ðŸŽ¯ The Streamlit app now has access to real ML predictions!\")\n",
        "\n",
        "# Show summary statistics\n",
        "print(\"\\nðŸ“ˆ Summary of ML Results:\")\n",
        "\n",
        "summary_stats = session.sql(\"\"\"\n",
        "    SELECT \n",
        "        COUNT(*) as TOTAL_USERS,\n",
        "        SUM(CASE WHEN NATIVE_IS_ANOMALY THEN 1 ELSE 0 END) as NATIVE_ANOMALIES,\n",
        "        SUM(CASE WHEN ISOLATION_FOREST_ANOMALY THEN 1 ELSE 0 END) as SNOWPARK_ANOMALIES,\n",
        "        SUM(CASE WHEN RISK_LEVEL = 'CRITICAL' THEN 1 ELSE 0 END) as CRITICAL_RISK,\n",
        "        SUM(CASE WHEN MODEL_AGREEMENT = 'BOTH_AGREE_ANOMALY' THEN 1 ELSE 0 END) as MODELS_AGREE\n",
        "    FROM ML_MODEL_COMPARISON\n",
        "\"\"\")\n",
        "\n",
        "summary_stats.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
