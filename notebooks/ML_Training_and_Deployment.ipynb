{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üõ°Ô∏è Cybersecurity ML Training & Deployment\n",
        "\n",
        "This notebook demonstrates advanced machine learning for cybersecurity using **Snowpark ML** and **Snowflake Model Registry**.\n",
        "\n",
        "## üéØ What This Notebook Does\n",
        "\n",
        "1. **Isolation Forest**: Detects anomalous user behavior patterns\n",
        "2. **K-means Clustering**: Groups users into behavioral personas  \n",
        "3. **Model Registry**: Enterprise-grade model management\n",
        "4. **UDF Deployment**: Deploy models as scalable functions\n",
        "5. **Hybrid Analysis**: Combine multiple ML approaches\n",
        "\n",
        "## üìã Prerequisites\n",
        "\n",
        "Before running this notebook, ensure you have:\n",
        "1. ‚úÖ Run `sql/01_cybersecurity_schema.sql` \n",
        "2. ‚úÖ Run `sql/02_sample_data_generation.sql`\n",
        "3. ‚úÖ Run `sql/03_native_ml_and_cortex.sql`\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import snowflake.snowpark as snowpark\n",
        "from snowflake.snowpark import Session\n",
        "from snowflake.snowpark.functions import col, lit, when, avg, count, max as max_, min as min_\n",
        "from snowflake.snowpark.types import StructType, StructField, StringType, FloatType, IntegerType, BooleanType\n",
        "\n",
        "# Snowpark ML imports\n",
        "from snowflake.ml.modeling.ensemble import IsolationForest\n",
        "from snowflake.ml.modeling.cluster import KMeans\n",
        "from snowflake.ml.modeling.preprocessing import StandardScaler\n",
        "from snowflake.ml.registry import Registry\n",
        "\n",
        "# Data science libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "\n",
        "print(\"üì¶ All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Snowflake session (auto-connects in Snowflake Notebooks)\n",
        "session = snowpark.context.get_active_session()\n",
        "\n",
        "# Set up database context\n",
        "session.sql(\"USE DATABASE CYBERSECURITY_DEMO\").collect()\n",
        "session.sql(\"USE SCHEMA SECURITY_ANALYTICS\").collect()\n",
        "session.sql(\"USE WAREHOUSE CYBERSECURITY_WH\").collect()\n",
        "\n",
        "print(\"üîó Connected to Snowflake session\")\n",
        "print(f\"üìä Current database: {session.get_current_database()}\")\n",
        "print(f\"üìÅ Current schema: {session.get_current_schema()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Data Preparation for ML\n",
        "\n",
        "*The ML models require feature engineering to extract behavioral patterns from raw authentication logs.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create feature engineering view for ML models with explicit type casting\n",
        "feature_query = \"\"\"\n",
        "CREATE OR REPLACE VIEW ML_FEATURE_SET AS\n",
        "SELECT \n",
        "    ual.USERNAME,\n",
        "    \n",
        "    -- Behavioral features (cast to avoid decimal precision warnings)\n",
        "    COUNT(*)::INTEGER as TOTAL_LOGINS,\n",
        "    AVG(CASE WHEN ual.SUCCESS THEN 1 ELSE 0 END)::FLOAT as SUCCESS_RATE,\n",
        "    COUNT(CASE WHEN ual.SUCCESS = FALSE THEN 1 END)::INTEGER as FAILED_ATTEMPTS,\n",
        "    \n",
        "    -- Temporal features\n",
        "    COUNT(CASE WHEN EXTRACT(hour FROM ual.TIMESTAMP) BETWEEN 22 AND 6 THEN 1 END)::INTEGER as OFF_HOURS_LOGINS,\n",
        "    COUNT(CASE WHEN EXTRACT(dow FROM ual.TIMESTAMP) IN (0,6) THEN 1 END)::INTEGER as WEEKEND_LOGINS,\n",
        "    \n",
        "    -- Geographic features\n",
        "    COUNT(DISTINCT ual.SOURCE_IP)::INTEGER as UNIQUE_IPS,\n",
        "    COUNT(DISTINCT ual.LOCATION:country::STRING)::INTEGER as UNIQUE_COUNTRIES,\n",
        "    \n",
        "    -- Security features\n",
        "    AVG(CASE WHEN ual.TWO_FACTOR_USED THEN 1 ELSE 0 END)::FLOAT as TWO_FACTOR_RATE,\n",
        "    COUNT(DISTINCT ual.USER_AGENT)::INTEGER as UNIQUE_DEVICES,\n",
        "    \n",
        "    -- Organizational context\n",
        "    ed.DEPARTMENT,\n",
        "    ed.ROLE,\n",
        "    ed.SECURITY_CLEARANCE,\n",
        "    DATEDIFF(day, ed.HIRE_DATE, CURRENT_DATE())::INTEGER as TENURE_DAYS\n",
        "    \n",
        "FROM USER_AUTHENTICATION_LOGS ual\n",
        "JOIN EMPLOYEE_DATA ed ON ual.USERNAME = ed.USERNAME\n",
        "WHERE ual.TIMESTAMP >= DATEADD(day, -45, CURRENT_TIMESTAMP())  -- Extended window for more data\n",
        "    AND ed.STATUS = 'active'\n",
        "GROUP BY ual.USERNAME, ed.DEPARTMENT, ed.ROLE, ed.SECURITY_CLEARANCE, ed.HIRE_DATE\n",
        "HAVING COUNT(*) >= 2  -- Filter users with minimum activity (reduced for demo)\n",
        "\"\"\"\n",
        "\n",
        "session.sql(feature_query).collect()\n",
        "print(\"üîß Feature engineering view created with explicit type casting\")\n",
        "\n",
        "# Load feature data into Snowpark DataFrame\n",
        "feature_df = session.table('ML_FEATURE_SET')\n",
        "print(f\"üìà Feature dataset: {feature_df.count()} users with behavioral features\")\n",
        "\n",
        "# Additional type casting in Snowpark DataFrame to ensure clean data types\n",
        "from snowflake.snowpark.types import IntegerType, FloatType\n",
        "\n",
        "# Cast numerical columns to appropriate types to prevent decimal conversion warnings\n",
        "feature_df = feature_df.with_column(\"TOTAL_LOGINS\", col(\"TOTAL_LOGINS\").cast(IntegerType())) \\\n",
        "                       .with_column(\"SUCCESS_RATE\", col(\"SUCCESS_RATE\").cast(FloatType())) \\\n",
        "                       .with_column(\"FAILED_ATTEMPTS\", col(\"FAILED_ATTEMPTS\").cast(IntegerType())) \\\n",
        "                       .with_column(\"OFF_HOURS_LOGINS\", col(\"OFF_HOURS_LOGINS\").cast(IntegerType())) \\\n",
        "                       .with_column(\"WEEKEND_LOGINS\", col(\"WEEKEND_LOGINS\").cast(IntegerType())) \\\n",
        "                       .with_column(\"UNIQUE_IPS\", col(\"UNIQUE_IPS\").cast(IntegerType())) \\\n",
        "                       .with_column(\"UNIQUE_COUNTRIES\", col(\"UNIQUE_COUNTRIES\").cast(IntegerType())) \\\n",
        "                       .with_column(\"TWO_FACTOR_RATE\", col(\"TWO_FACTOR_RATE\").cast(FloatType())) \\\n",
        "                       .with_column(\"UNIQUE_DEVICES\", col(\"UNIQUE_DEVICES\").cast(IntegerType())) \\\n",
        "                       .with_column(\"TENURE_DAYS\", col(\"TENURE_DAYS\").cast(IntegerType()))\n",
        "\n",
        "print(\"‚úÖ Data types explicitly cast to prevent precision warnings\")\n",
        "\n",
        "# Inspect and display data types to ensure all are correct\n",
        "print(\"\\nüîç Final data types inspection:\")\n",
        "feature_df.dtypes\n",
        "\n",
        "# Show sample of features with clean data types\n",
        "print(\"\\nüìä Sample of clean feature data:\")\n",
        "feature_df.limit(5).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Isolation Forest - Anomaly Detection\n",
        "\n",
        "*Uses unsupervised learning to identify users with anomalous behavior patterns that could indicate security threats.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative Isolation Forest Implementation (Fallback)\n",
        "# This provides a working implementation if the Snowpark ML version has issues\n",
        "\n",
        "def create_statistical_anomaly_scores(df, features):\n",
        "    \"\"\"\n",
        "    Create anomaly scores using statistical methods as a fallback\n",
        "    \"\"\"\n",
        "    print(\"üîÑ Using statistical anomaly detection as fallback...\")\n",
        "    \n",
        "    # Calculate z-scores for each feature and combine them\n",
        "    anomaly_df = df\n",
        "    \n",
        "    for feature in features:\n",
        "        # Calculate mean and std for the feature\n",
        "        stats = df.select(\n",
        "            avg(col(feature)).alias('mean_val'),\n",
        "            stddev(col(feature)).alias('std_val')\n",
        "        ).collect()[0]\n",
        "        \n",
        "        mean_val = float(stats['MEAN_VAL']) if stats['MEAN_VAL'] is not None else 0.0\n",
        "        std_val = float(stats['STD_VAL']) if stats['STD_VAL'] is not None else 1.0\n",
        "        \n",
        "        # Calculate z-score for this feature\n",
        "        anomaly_df = anomaly_df.with_column(\n",
        "            f'{feature}_ZSCORE',\n",
        "            abs((col(feature) - lit(mean_val)) / lit(std_val))\n",
        "        )\n",
        "    \n",
        "    # Combine z-scores into an anomaly score\n",
        "    zscore_cols = [f'{feature}_ZSCORE' for feature in features]\n",
        "    anomaly_df = anomaly_df.with_column(\n",
        "        'ISOLATION_ANOMALY_SCORE',\n",
        "        -(sqrt(sum([col(zs) ** 2 for zs in zscore_cols]) / lit(len(zscore_cols))))\n",
        "    )\n",
        "    \n",
        "    # Clean up temporary columns\n",
        "    for zs_col in zscore_cols:\n",
        "        anomaly_df = anomaly_df.drop(zs_col)\n",
        "    \n",
        "    print(\"‚úÖ Statistical anomaly scores calculated!\")\n",
        "    return anomaly_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to ensure clean data types and prevent decimal warnings\n",
        "def ensure_clean_datatypes(df, numerical_columns):\n",
        "    \"\"\"Ensures all numerical columns have appropriate data types for ML\"\"\"\n",
        "    from snowflake.snowpark.types import IntegerType, FloatType, DecimalType\n",
        "    \n",
        "    for col_name in numerical_columns:\n",
        "        current_type = dict(df.dtypes)[col_name]\n",
        "        print(f\"  {col_name}: {current_type}\")\n",
        "        \n",
        "        # Convert any remaining decimal types to appropriate types\n",
        "        if isinstance(current_type, DecimalType):\n",
        "            # Scaled features (ending with _SCALED) should always be FloatType\n",
        "            if col_name.endswith('_SCALED') or col_name in ['SUCCESS_RATE', 'TWO_FACTOR_RATE']:\n",
        "                df = df.with_column(col_name, col(col_name).cast(FloatType()))\n",
        "                print(f\"    ‚Üí Converted {col_name} to FloatType\")\n",
        "            else:\n",
        "                df = df.with_column(col_name, col(col_name).cast(IntegerType()))\n",
        "                print(f\"    ‚Üí Converted {col_name} to IntegerType\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Prepare numerical features for Isolation Forest\n",
        "iso_features = [\n",
        "    'TOTAL_LOGINS', 'SUCCESS_RATE', 'FAILED_ATTEMPTS', \n",
        "    'OFF_HOURS_LOGINS', 'WEEKEND_LOGINS', 'UNIQUE_IPS', \n",
        "    'UNIQUE_COUNTRIES', 'TWO_FACTOR_RATE', 'UNIQUE_DEVICES', 'TENURE_DAYS'\n",
        "]\n",
        "\n",
        "print(\"üîß Ensuring clean data types for ML features:\")\n",
        "feature_df = ensure_clean_datatypes(feature_df, iso_features)\n",
        "\n",
        "# Create Isolation Forest model\n",
        "# Note: Snowpark ML IsolationForest returns anomaly scores only, not binary flags\n",
        "isolation_forest = IsolationForest(\n",
        "    n_estimators=100,\n",
        "    contamination=0.1,  # Expect ~10% anomalies  \n",
        "    random_state=42,\n",
        "    input_cols=iso_features,\n",
        "    output_cols=['ISOLATION_ANOMALY_SCORE']  # Single output column for anomaly scores\n",
        ")\n",
        "\n",
        "print(\"üå≤ Training Isolation Forest model...\")\n",
        "\n",
        "# Debug: Check input data shape and columns\n",
        "print(f\"üìä Input data shape: {feature_df.count()} rows\")\n",
        "print(f\"üìã Input columns: {iso_features}\")\n",
        "print(f\"üéØ Expected output columns: {isolation_forest.get_output_cols()}\")\n",
        "\n",
        "# Train the model\n",
        "isolation_model = isolation_forest.fit(feature_df)\n",
        "\n",
        "print(\"‚úÖ Isolation Forest training completed!\")\n",
        "print(f\"üîç Model output columns: {isolation_model.get_output_cols()}\")\n",
        "\n",
        "# Apply predictions to the dataset with error handling\n",
        "print(\"üîÆ Applying Isolation Forest predictions...\")\n",
        "try:\n",
        "    feature_df_with_iso = isolation_model.predict(feature_df)\n",
        "    print(\"‚úÖ Predictions applied successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error during prediction: {str(e)}\")\n",
        "    print(\"üîß Attempting alternative approaches...\")\n",
        "    \n",
        "    # Alternative 1: Recreate the model with explicit single output\n",
        "    try:\n",
        "        print(\"üîÑ Attempting approach 1: Recreating Isolation Forest...\")\n",
        "        isolation_forest_fixed = IsolationForest(\n",
        "            n_estimators=100,\n",
        "            contamination=0.1,\n",
        "            random_state=42,\n",
        "            input_cols=iso_features,\n",
        "            output_cols=['ANOMALY_SCORE']  # Different column name to avoid conflicts\n",
        "        )\n",
        "        \n",
        "        isolation_model_fixed = isolation_forest_fixed.fit(feature_df)\n",
        "        feature_df_with_iso = isolation_model_fixed.predict(feature_df)\n",
        "        \n",
        "        # Rename the column to match expected name\n",
        "        feature_df_with_iso = feature_df_with_iso.with_column_renamed('ANOMALY_SCORE', 'ISOLATION_ANOMALY_SCORE')\n",
        "        print(\"‚úÖ Alternative approach 1 successful!\")\n",
        "        \n",
        "    except Exception as e2:\n",
        "        print(f\"‚ùå Approach 1 also failed: {str(e2)}\")\n",
        "        print(\"üîÑ Attempting approach 2: Statistical anomaly detection...\")\n",
        "        \n",
        "        # Alternative 2: Use statistical anomaly detection\n",
        "        feature_df_with_iso = create_statistical_anomaly_scores(feature_df, iso_features)\n",
        "        print(\"‚úÖ Statistical anomaly detection successful!\")\n",
        "\n",
        "# Create binary anomaly flag from anomaly scores\n",
        "# Isolation Forest returns negative scores for anomalies (lower scores = more anomalous)\n",
        "feature_df_with_iso = feature_df_with_iso.with_column(\n",
        "    'ISOLATION_IS_ANOMALY', \n",
        "    when(col('ISOLATION_ANOMALY_SCORE') < -0.1, lit(True)).otherwise(lit(False))\n",
        ")\n",
        "\n",
        "print(\"üîç Anomaly detection results:\")\n",
        "anomaly_summary = feature_df_with_iso.group_by('ISOLATION_IS_ANOMALY').agg(count(lit(1)).alias('COUNT'))\n",
        "anomaly_summary.show()\n",
        "\n",
        "# Show distribution of anomaly scores\n",
        "print(\"\\nüìä Anomaly score distribution:\")\n",
        "score_stats = feature_df_with_iso.select(\n",
        "    min_('ISOLATION_ANOMALY_SCORE').alias('MIN_SCORE'),\n",
        "    avg('ISOLATION_ANOMALY_SCORE').alias('AVG_SCORE'), \n",
        "    max_('ISOLATION_ANOMALY_SCORE').alias('MAX_SCORE')\n",
        ")\n",
        "score_stats.show()\n",
        "\n",
        "# Show sample anomalies\n",
        "print(\"\\nüö® Sample anomalous users:\")\n",
        "anomalies = feature_df_with_iso.filter(col('ISOLATION_IS_ANOMALY') == True).select(\n",
        "    'USERNAME', 'DEPARTMENT', 'TOTAL_LOGINS', 'SUCCESS_RATE', \n",
        "    'UNIQUE_COUNTRIES', 'ISOLATION_ANOMALY_SCORE'\n",
        ").limit(10)\n",
        "anomalies.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üîç Data Validation & Troubleshooting\n",
        "\n",
        "*Validate that we have sufficient data for ML training before proceeding with clustering.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data validation before ML training\n",
        "print(\"üîç Validating data availability for ML training...\")\n",
        "\n",
        "# Check feature dataset size\n",
        "feature_count = feature_df_with_iso.count()\n",
        "print(f\"üìä Total users with features: {feature_count}\")\n",
        "\n",
        "# Check data distribution\n",
        "if feature_count == 0:\n",
        "    print(\"‚ùå ERROR: No users found in feature dataset!\")\n",
        "    print(\"üîß Troubleshooting steps:\")\n",
        "    print(\"   1. Verify that sample data generation (02_sample_data_generation.sql) was run\")\n",
        "    print(\"   2. Check that authentication logs have recent timestamps\")\n",
        "    print(\"   3. Ensure employee data has active users\")\n",
        "    raise Exception(\"Insufficient data for ML training\")\n",
        "elif feature_count < 4:\n",
        "    print(f\"‚ö†Ô∏è  WARNING: Only {feature_count} users available for clustering\")\n",
        "    print(\"üîß Consider running with more sample data for better clustering results\")\n",
        "else:\n",
        "    print(f\"‚úÖ Sufficient data available: {feature_count} users\")\n",
        "\n",
        "# Show sample of feature data for verification\n",
        "print(\"\\nüìã Sample feature data:\")\n",
        "feature_df_with_iso.select(\n",
        "    'USERNAME', 'DEPARTMENT', 'TOTAL_LOGINS', 'SUCCESS_RATE', \n",
        "    'UNIQUE_COUNTRIES', 'ISOLATION_IS_ANOMALY'\n",
        ").show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üë• K-means Clustering - User Personas\n",
        "\n",
        "*Groups users into behavioral clusters to understand normal patterns and identify deviations from expected behavior.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features for clustering (normalize for better clustering)\n",
        "cluster_features = [\n",
        "    'TOTAL_LOGINS', 'SUCCESS_RATE', 'OFF_HOURS_LOGINS', \n",
        "    'WEEKEND_LOGINS', 'UNIQUE_IPS', 'UNIQUE_COUNTRIES', 'TWO_FACTOR_RATE'\n",
        "]\n",
        "\n",
        "print(\"üîß Ensuring clean data types for clustering features:\")\n",
        "feature_df_with_iso = ensure_clean_datatypes(feature_df_with_iso, cluster_features)\n",
        "\n",
        "# Scale features for clustering\n",
        "scaler = StandardScaler(\n",
        "    input_cols=cluster_features,\n",
        "    output_cols=[f\"{col}_SCALED\" for col in cluster_features]\n",
        ")\n",
        "\n",
        "scaled_df = scaler.fit(feature_df_with_iso).transform(feature_df_with_iso)\n",
        "\n",
        "# Fix decimal precision warnings from StandardScaler output\n",
        "print(\"üîß Ensuring clean data types for scaled features:\")\n",
        "scaled_feature_cols = [f\"{col}_SCALED\" for col in cluster_features]\n",
        "scaled_df = ensure_clean_datatypes(scaled_df, scaled_feature_cols)\n",
        "\n",
        "# Create K-means clustering model with dynamic cluster count\n",
        "# First check the number of samples to ensure we have enough data\n",
        "sample_count = scaled_df.count()\n",
        "print(f\"üìä Sample count for clustering: {sample_count}\")\n",
        "\n",
        "# Dynamically adjust cluster count based on available data\n",
        "# Rule: need at least 2 samples per cluster for meaningful clustering\n",
        "max_clusters = max(1, min(4, sample_count // 2))\n",
        "actual_clusters = max_clusters if sample_count >= 4 else min(sample_count, 2)\n",
        "\n",
        "print(f\"üéØ Using {actual_clusters} clusters for {sample_count} samples\")\n",
        "\n",
        "kmeans = KMeans(\n",
        "    n_clusters=actual_clusters,  # Dynamic clusters based on data availability\n",
        "    random_state=42,\n",
        "    input_cols=[f\"{col}_SCALED\" for col in cluster_features],\n",
        "    output_cols=['CLUSTER_LABEL', 'CLUSTER_DISTANCE']\n",
        ")\n",
        "\n",
        "print(\"üîÑ Training K-means clustering model...\")\n",
        "\n",
        "# Train the clustering model\n",
        "kmeans_model = kmeans.fit(scaled_df)\n",
        "\n",
        "print(\"‚úÖ K-means clustering completed!\")\n",
        "\n",
        "# Apply clustering\n",
        "final_df = kmeans_model.predict(scaled_df)\n",
        "\n",
        "print(\"üë• User cluster distribution:\")\n",
        "\n",
        "# Apply type casting to prevent decimal precision warnings in aggregations\n",
        "print(\"üîß Ensuring clean data types for cluster analysis...\")\n",
        "final_df_clean = final_df.with_column('TOTAL_LOGINS', col('TOTAL_LOGINS').cast(IntegerType())) \\\n",
        "                         .with_column('SUCCESS_RATE', col('SUCCESS_RATE').cast(FloatType())) \\\n",
        "                         .with_column('UNIQUE_COUNTRIES', col('UNIQUE_COUNTRIES').cast(IntegerType()))\n",
        "\n",
        "cluster_summary = final_df_clean.group_by('CLUSTER_LABEL').agg(\n",
        "    count(lit(1)).alias('USER_COUNT'),\n",
        "    avg('TOTAL_LOGINS').alias('AVG_LOGINS'),\n",
        "    avg('SUCCESS_RATE').alias('AVG_SUCCESS_RATE'),\n",
        "    avg('UNIQUE_COUNTRIES').alias('AVG_COUNTRIES')\n",
        ")\n",
        "cluster_summary.show()\n",
        "\n",
        "# Analyze cluster characteristics\n",
        "print(\"\\nüìä Cluster characteristics by department:\")\n",
        "\n",
        "# Get actual cluster labels from the data\n",
        "cluster_labels = [str(i) for i in range(actual_clusters)]\n",
        "print(f\"üìã Cluster labels: {cluster_labels}\")\n",
        "\n",
        "if actual_clusters > 1:\n",
        "    # Only do pivot analysis if we have multiple clusters\n",
        "    agg_dict = {str(i): 'sum' for i in range(actual_clusters)}\n",
        "    \n",
        "    dept_clusters = final_df_clean.group_by('DEPARTMENT', 'CLUSTER_LABEL').agg(\n",
        "        count(lit(1)).alias('COUNT')\n",
        "    ).pivot('CLUSTER_LABEL', cluster_labels).agg(agg_dict).fillna(0)\n",
        "    dept_clusters.show()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Only one cluster created - no department pivot analysis available\")\n",
        "    simple_dept = final_df_clean.group_by('DEPARTMENT').agg(count(lit(1)).alias('USER_COUNT'))\n",
        "    simple_dept.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Model Registry - Enterprise ML Management\n",
        "\n",
        "*Deploy trained models to Snowflake Model Registry for enterprise-grade model lifecycle management.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Model Registry\n",
        "registry = Registry(session=session, database_name=\"CYBERSECURITY_DEMO\", schema_name=\"SECURITY_ANALYTICS\")\n",
        "\n",
        "print(\"üìã Registering models in Snowflake Model Registry...\")\n",
        "\n",
        "# Register Isolation Forest model\n",
        "iso_model_ref = registry.log_model(\n",
        "    model=isolation_model,\n",
        "    model_name=\"cybersecurity_isolation_forest\",\n",
        "    version_name=\"v1.0\",\n",
        "    comment=\"Isolation Forest for detecting anomalous user behavior patterns\",\n",
        "    tags={\"use_case\": \"anomaly_detection\", \"model_type\": \"isolation_forest\", \"department\": \"security\"}\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Isolation Forest registered: {iso_model_ref.fully_qualified_model_name}\")\n",
        "\n",
        "# Register K-means model  \n",
        "kmeans_model_ref = registry.log_model(\n",
        "    model=kmeans_model,\n",
        "    model_name=\"cybersecurity_user_clustering\", \n",
        "    version_name=\"v1.0\",\n",
        "    comment=\"K-means clustering for user behavioral personas\",\n",
        "    tags={\"use_case\": \"user_clustering\", \"model_type\": \"kmeans\", \"department\": \"security\"}\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ K-means model registered: {kmeans_model_ref.fully_qualified_model_name}\")\n",
        "\n",
        "# Register feature scaler\n",
        "scaler_ref = registry.log_model(\n",
        "    model=scaler,\n",
        "    model_name=\"cybersecurity_feature_scaler\",\n",
        "    version_name=\"v1.0\", \n",
        "    comment=\"StandardScaler for normalizing cybersecurity features\",\n",
        "    tags={\"use_case\": \"preprocessing\", \"model_type\": \"scaler\", \"department\": \"security\"}\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Feature scaler registered: {scaler_ref.fully_qualified_model_name}\")\n",
        "\n",
        "# List all registered models\n",
        "print(\"\\nüìö All models in registry:\")\n",
        "models = registry.list_models()\n",
        "for model in models:\n",
        "    print(f\"  ü§ñ {model}\")\n",
        "\n",
        "print(\"\\nüéØ Model Registry deployment completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö° UDF Deployment - Production ML Functions\n",
        "\n",
        "*Deploy models as SQL UDFs for real-time scoring in production workloads.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deploy Isolation Forest as UDF for real-time anomaly detection\n",
        "iso_udf = iso_model_ref.run(\n",
        "    X=session.table('ML_FEATURE_SET').select(iso_features),\n",
        "    function_name=\"DETECT_USER_ANOMALIES\"\n",
        ")\n",
        "\n",
        "print(\"üö® Isolation Forest UDF deployed: DETECT_USER_ANOMALIES\")\n",
        "\n",
        "# Deploy K-means clustering as UDF for user segmentation  \n",
        "cluster_udf = kmeans_model_ref.run(\n",
        "    X=scaler_ref.run(session.table('ML_FEATURE_SET').select(cluster_features)),\n",
        "    function_name=\"CLASSIFY_USER_BEHAVIOR\"\n",
        ")\n",
        "\n",
        "print(\"üë• K-means clustering UDF deployed: CLASSIFY_USER_BEHAVIOR\")\n",
        "\n",
        "# Test the deployed UDFs\n",
        "print(\"\\nüß™ Testing deployed UDFs...\")\n",
        "\n",
        "# Test anomaly detection UDF\n",
        "anomaly_test = session.sql(\"\"\"\n",
        "    SELECT \n",
        "        USERNAME,\n",
        "        DEPARTMENT,\n",
        "        TOTAL_LOGINS,\n",
        "        DETECT_USER_ANOMALIES(\n",
        "            TOTAL_LOGINS, SUCCESS_RATE, FAILED_ATTEMPTS, OFF_HOURS_LOGINS,\n",
        "            WEEKEND_LOGINS, UNIQUE_IPS, UNIQUE_COUNTRIES, TWO_FACTOR_RATE,\n",
        "            UNIQUE_DEVICES, TENURE_DAYS\n",
        "        ) as ANOMALY_PREDICTION\n",
        "    FROM ML_FEATURE_SET\n",
        "    LIMIT 5\n",
        "\"\"\")\n",
        "\n",
        "print(\"üîç Anomaly Detection UDF test:\")\n",
        "anomaly_test.show()\n",
        "\n",
        "# Test clustering UDF  \n",
        "cluster_test = session.sql(\"\"\"\n",
        "    SELECT \n",
        "        USERNAME,\n",
        "        DEPARTMENT, \n",
        "        CLASSIFY_USER_BEHAVIOR(\n",
        "            TOTAL_LOGINS, SUCCESS_RATE, OFF_HOURS_LOGINS,\n",
        "            WEEKEND_LOGINS, UNIQUE_IPS, UNIQUE_COUNTRIES, TWO_FACTOR_RATE\n",
        "        ) as CLUSTER_PREDICTION\n",
        "    FROM ML_FEATURE_SET\n",
        "    LIMIT 5\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nüë• User Clustering UDF test:\")\n",
        "cluster_test.show()\n",
        "\n",
        "print(\"\\n‚úÖ UDF deployment and testing completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Update Snowpark ML Results Tables\n",
        "\n",
        "*Populate the ML results tables that the Streamlit app uses for hybrid analysis.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Snowpark ML anomaly results table\n",
        "print(\"üìä Creating SNOWPARK_ML_ANOMALY_RESULTS table...\")\n",
        "\n",
        "snowpark_anomaly_query = \"\"\"\n",
        "CREATE OR REPLACE TABLE SNOWPARK_ML_ANOMALY_RESULTS AS\n",
        "SELECT \n",
        "    USERNAME,\n",
        "    ISOLATION_ANOMALY_SCORE,\n",
        "    ISOLATION_IS_ANOMALY,\n",
        "    CURRENT_TIMESTAMP() as ANALYSIS_TIMESTAMP\n",
        "FROM ML_FEATURE_SET f\n",
        "JOIN ({}) ml ON f.USERNAME = ml.USERNAME\n",
        "\"\"\".format(\n",
        "    final_df.select('USERNAME', 'ISOLATION_ANOMALY_SCORE', 'ISOLATION_IS_ANOMALY').queries['queries'][0]\n",
        ")\n",
        "\n",
        "session.sql(snowpark_anomaly_query).collect()\n",
        "\n",
        "# Create Snowpark ML clustering results table\n",
        "print(\"üë• Creating SNOWPARK_ML_USER_CLUSTERS table...\")\n",
        "\n",
        "snowpark_cluster_query = \"\"\"\n",
        "CREATE OR REPLACE TABLE SNOWPARK_ML_USER_CLUSTERS AS\n",
        "SELECT \n",
        "    f.USERNAME,\n",
        "    f.DEPARTMENT,\n",
        "    f.TOTAL_LOGINS,\n",
        "    f.SUCCESS_RATE,\n",
        "    f.UNIQUE_COUNTRIES,\n",
        "    f.WEEKEND_LOGINS,\n",
        "    ml.CLUSTER_LABEL,\n",
        "    ml.CLUSTER_DISTANCE,\n",
        "    CURRENT_TIMESTAMP() as ANALYSIS_TIMESTAMP\n",
        "FROM ML_FEATURE_SET f\n",
        "JOIN ({}) ml ON f.USERNAME = ml.USERNAME\n",
        "\"\"\".format(\n",
        "    final_df.select('USERNAME', 'CLUSTER_LABEL', 'CLUSTER_DISTANCE').queries['queries'][0]\n",
        ")\n",
        "\n",
        "session.sql(snowpark_cluster_query).collect()\n",
        "\n",
        "# Update the ML_MODEL_COMPARISON view with real Snowpark ML results\n",
        "print(\"üîÑ Updating ML_MODEL_COMPARISON view...\")\n",
        "\n",
        "update_comparison_view = \"\"\"\n",
        "CREATE OR REPLACE VIEW ML_MODEL_COMPARISON AS\n",
        "SELECT\n",
        "    n.USERNAME,\n",
        "    ed.DEPARTMENT,\n",
        "    ed.ROLE,\n",
        "    CURRENT_TIMESTAMP() as ANALYSIS_DATE,\n",
        "\n",
        "    -- Native ML Results \n",
        "    COALESCE(n.IS_ANOMALY, FALSE) as NATIVE_IS_ANOMALY,\n",
        "    COALESCE(n.FORECAST, 0) as NATIVE_ANOMALY_SCORE,\n",
        "\n",
        "    -- Snowpark ML Results (now with real data!)\n",
        "    COALESCE(s.ISOLATION_IS_ANOMALY, FALSE) as ISOLATION_FOREST_ANOMALY,\n",
        "    COALESCE(s.ISOLATION_ANOMALY_SCORE, 0.0) as ISOLATION_FOREST_SCORE,\n",
        "    COALESCE(c.CLUSTER_LABEL, 0) as CLUSTER_LABEL,\n",
        "    COALESCE(c.CLUSTER_DISTANCE, 0.0) as CLUSTER_DISTANCE,\n",
        "\n",
        "    -- Enhanced Risk Assessment using both models\n",
        "    CASE\n",
        "        WHEN COALESCE(n.IS_ANOMALY, FALSE) = TRUE AND COALESCE(s.ISOLATION_IS_ANOMALY, FALSE) = TRUE THEN 'CRITICAL'\n",
        "        WHEN COALESCE(n.IS_ANOMALY, FALSE) = TRUE OR COALESCE(s.ISOLATION_IS_ANOMALY, FALSE) = TRUE THEN 'HIGH'\n",
        "        WHEN n.FORECAST IS NOT NULL OR s.ISOLATION_ANOMALY_SCORE > 0.5 THEN 'MEDIUM'\n",
        "        ELSE 'LOW'\n",
        "    END as RISK_LEVEL,\n",
        "\n",
        "    -- Model agreement analysis\n",
        "    CASE\n",
        "        WHEN COALESCE(n.IS_ANOMALY, FALSE) = TRUE AND COALESCE(s.ISOLATION_IS_ANOMALY, FALSE) = TRUE THEN 'BOTH_AGREE_ANOMALY'\n",
        "        WHEN COALESCE(n.IS_ANOMALY, FALSE) = FALSE AND COALESCE(s.ISOLATION_IS_ANOMALY, FALSE) = FALSE THEN 'BOTH_AGREE_NORMAL'\n",
        "        WHEN COALESCE(n.IS_ANOMALY, FALSE) = TRUE THEN 'NATIVE_ONLY'\n",
        "        WHEN COALESCE(s.ISOLATION_IS_ANOMALY, FALSE) = TRUE THEN 'SNOWPARK_ONLY'\n",
        "        ELSE 'INSUFFICIENT_DATA'\n",
        "    END as MODEL_AGREEMENT\n",
        "\n",
        "FROM NATIVE_ML_ANOMALY_RESULTS n\n",
        "FULL OUTER JOIN SNOWPARK_ML_ANOMALY_RESULTS s ON n.USERNAME = s.USERNAME  \n",
        "FULL OUTER JOIN SNOWPARK_ML_USER_CLUSTERS c ON COALESCE(n.USERNAME, s.USERNAME) = c.USERNAME\n",
        "JOIN EMPLOYEE_DATA ed ON COALESCE(n.USERNAME, s.USERNAME, c.USERNAME) = ed.USERNAME\n",
        "WHERE ed.STATUS = 'active'\n",
        "\"\"\"\n",
        "\n",
        "session.sql(update_comparison_view).collect()\n",
        "\n",
        "print(\"‚úÖ Snowpark ML tables created and ML_MODEL_COMPARISON view updated!\")\n",
        "print(\"üéØ The Streamlit app now has access to real ML predictions!\")\n",
        "\n",
        "# Show summary statistics\n",
        "print(\"\\nüìà Summary of ML Results:\")\n",
        "\n",
        "summary_stats = session.sql(\"\"\"\n",
        "    SELECT \n",
        "        COUNT(*) as TOTAL_USERS,\n",
        "        SUM(CASE WHEN NATIVE_IS_ANOMALY THEN 1 ELSE 0 END) as NATIVE_ANOMALIES,\n",
        "        SUM(CASE WHEN ISOLATION_FOREST_ANOMALY THEN 1 ELSE 0 END) as SNOWPARK_ANOMALIES,\n",
        "        SUM(CASE WHEN RISK_LEVEL = 'CRITICAL' THEN 1 ELSE 0 END) as CRITICAL_RISK,\n",
        "        SUM(CASE WHEN MODEL_AGREEMENT = 'BOTH_AGREE_ANOMALY' THEN 1 ELSE 0 END) as MODELS_AGREE\n",
        "    FROM ML_MODEL_COMPARISON\n",
        "\"\"\")\n",
        "\n",
        "summary_stats.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
